{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next I'll load my data set called tree_addhealth.csv. because decision tree analyses cannot handle any NAs in our data set, my next step is to create a clean data frame that drops all NAs. Setting the new data frame called data_clean I can now take a look at various characteristics of my data, by using the D types and describe functions to examine data types and summary statistics. Next I set my explanatory and response, or target variables, and then include the train test split function for predictors and target. And set the size ratio to 60% for the training sample, and 40% for the test sample by indicating test_size=.4. Here I request the shape of these predictor and target and training test samples. From sklearn.ensamble I import the RandomForestClassifier. Now that training and test data sets have already been created, we'll initialize the random forest classifier from SK Learn and indicate n_estimators=25. n_estimators are the number of trees you would build with the random forest algorithm. \n",
    "****\n",
    "Next I actually fit the model with the classifier.fit function which we passed the training predictors and training targets too. \n",
    "****\n",
    "Then, we go unto the prediction on the testator set. And we could also similar to decision tree code as for the confusion matrix and accuracy scores. For the confusion matrix, we see the true negatives and true positives on the diagonal. And the 207 and the 82 represent the false negatives and false positives, respectively. Notice that the overall accuracy for the forest is 0.84. So 84% of the individuals were classified correctly, as regular smokers, or not regular smokers. Given that we don't interpret individual trees in a random forest, the most helpful information to be gotten from a forest is arguably the measured importance for each explanatory variable. Also called the features. Based on how many votes or splits each has produced in the 25 tree ensemble. To generate importance scores, we initialize the extra tree classifier, and then fit a model. Finally, we ask Python to print the feature importance scores calculated from the forest of trees that we've grown. The variables are listed in the order they've been named earlier in the code. Starting with gender, called BIO_SEX, and ending with parental presence. As we can see the variables with the highest important score at 0.13 is marijuana use. And the variable with the lowest important score is Asian ethnicity at .006. As you will recall, the correct classification rate for the random forest was 84%. So were 25 trees actually needed to get this correct rate of classification? To determine what growing larger number of trees has brought us in terms of correct classification. We're going to use code that builds for us different numbers of trees, from one to 25, and provides the correct classification rate for each. This code will build for us random forest classifier from one to 25, and then finding the accuracy score for each of those trees from one to 25, and storing it in an array. This will give me 25 different accuracy values. And we'll plot them as the number of trees increase. As you can see, with only one tree the accuracy is about 83%, and it climbs to only about 84% with successive trees that are grown giving us some confidence that it may be perfectly appropriate to interpret a single decision tree for this data. Given that it's accuracy is quite near that of successive trees in the forest. To summarize, like decision trees, random forests are a type of data mining algorithm that can select from among a large number of variables. Those that are most important in determining the target or response variable to be explained. \n",
    "*****\n",
    "Also light decision trees. The target variable in a random forest can be categorical or quantitative. And the group of explanatory variables or features can be categorical and quantitative in any combination. Unlike decision trees however, the results of random forests often generalize well to new data. Since the strongest signals are able to emerge through the growing of many trees. Further, small changes in the data do not impact the results of a random forest. In my opinion, the main weakness of random forests is simply that the results are less satisfying, since no trees are actually interpreted. Instead, the forest of trees is used to rank the importance of variables in predicting the target. Thus we get a sense of the most important predictive variables but not necessarily their relationships to one another. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02490987  0.0141488   0.03319307  0.0159277   0.00918819  0.00458466\n",
      "  0.06601147  0.04622144  0.04057803  0.12496822  0.01263702  0.01503603\n",
      "  0.03037218  0.05640274  0.05690262  0.05217563  0.01818411  0.07851001\n",
      "  0.0609397   0.07078293  0.00821716  0.0560744   0.05690139  0.04713262]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c8e2359b00>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    " # Feature Importance\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#Load the dataset\n",
    "\n",
    "AH_data = pd.read_csv(r\"C:\\Users\\piush\\Desktop\\Dataset\\tree_addhealth.csv\")\n",
    "data_clean = AH_data.dropna()\n",
    "\n",
    "data_clean.dtypes\n",
    "data_clean.describe()\n",
    "\n",
    "#Split into training and testing sets\n",
    "\n",
    "predictors = data_clean[['BIO_SEX','HISPANIC','WHITE','BLACK','NAMERICAN','ASIAN','age',\n",
    "'ALCEVR1','ALCPROBS1','marever1','cocever1','inhever1','cigavail','DEP1','ESTEEM1','VIOL1',\n",
    "'PASSIST','DEVIANT1','SCHCONN1','GPA1','EXPEL1','FAMCONCT','PARACTV','PARPRES']]\n",
    "\n",
    "targets = data_clean.TREG1\n",
    "\n",
    "pred_train, pred_test, tar_train, tar_test  = train_test_split(predictors, targets, test_size=.4)\n",
    "\n",
    "pred_train.shape\n",
    "pred_test.shape\n",
    "tar_train.shape\n",
    "tar_test.shape\n",
    "\n",
    "#Build model on training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier=RandomForestClassifier(n_estimators=25)\n",
    "classifier=classifier.fit(pred_train,tar_train)\n",
    "\n",
    "predictions=classifier.predict(pred_test)\n",
    "\n",
    "sklearn.metrics.confusion_matrix(tar_test,predictions)\n",
    "sklearn.metrics.accuracy_score(tar_test, predictions)\n",
    "\n",
    "\n",
    "# fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(pred_train,tar_train)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Running a different number of trees and see the effect\n",
    " of that on the accuracy of the prediction\n",
    "\"\"\"\n",
    "\n",
    "trees=range(25)\n",
    "accuracy=np.zeros(25)\n",
    "\n",
    "for idx in range(len(trees)):\n",
    "   classifier=RandomForestClassifier(n_estimators=idx + 1)\n",
    "   classifier=classifier.fit(pred_train,tar_train)\n",
    "   predictions=classifier.predict(pred_test)\n",
    "   accuracy[idx]=sklearn.metrics.accuracy_score(tar_test, predictions)\n",
    "   \n",
    "plt.cla()\n",
    "plt.plot(trees, accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
