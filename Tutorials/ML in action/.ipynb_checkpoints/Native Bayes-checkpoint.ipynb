{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes\n",
    "\n",
    "Pros: Works with a small amount of data, handles multiple classes\n",
    "\n",
    "Cons: Sensitive to how the input data is prepared\n",
    "\n",
    "Works with: Nominal values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General approach to naïve Bayes\n",
    "\n",
    "1. Collect: Any method.\n",
    "    \n",
    "2. Prepare: Numeric or Boolean values are needed.\n",
    "\n",
    "3. Analyze: With many features, plotting features isn’t helpful. Looking at histograms is a better idea.\n",
    "\n",
    "4. Train: Calculate the conditional probabilities of the independent features.\n",
    "\n",
    "5. Test: Calculate the error rate.\n",
    "\n",
    "6. Use: One common application of naïve Bayes is document classification. You\n",
    "can use naïve Bayes in any classification setting. It doesn’t have to be text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make a quick filter for an online message board that flags\n",
    "a message as inappropriate if the author uses negative or abusive language. Filtering\n",
    "out this sort of thing is common because abusive postings make people not come back\n",
    "and can hurt an online community. We’ll have two categories: abusive and not. We’ll\n",
    "use 1 to represent abusive and 0 to represent not abusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we’re going to show how to transform lists of text into a vector of numbers.\n",
    "Next, we’ll show how to calculate conditional probabilities from these vectors. Then,\n",
    "we’ll create a classifier, and finally, we’ll look at some practical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare: making word vectors from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text has been labeled by a\n",
    "human and will be used to train a program to automatically detect abusive posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function createVocabList() will create a list of all the unique words in all\n",
    "of our documents. To create this unique list you use the Python set data type. You can\n",
    "give a list of items to the set constructor, and it will only return a unique list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        #The | operator is used for union of two sets; recall that this is the bitwise OR operator\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function\n",
    "setOfWords2Vec(), which takes the vocabulary list and a document and outputs a vector\n",
    "of 1s and 0s to represent whether a word from our vocabulary is present or not in\n",
    "the given document. You then create a vector the same length as the vocabulary list and\n",
    "fill it up with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print (\"the word: %s is not in my Vocabulary!\") % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listOposts,listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myVocabList = createVocabList(listOposts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flea',\n",
       " 'posting',\n",
       " 'to',\n",
       " 'steak',\n",
       " 'mr',\n",
       " 'not',\n",
       " 'stupid',\n",
       " 'help',\n",
       " 'has',\n",
       " 'quit',\n",
       " 'garbage',\n",
       " 'him',\n",
       " 'park',\n",
       " 'so',\n",
       " 'dalmation',\n",
       " 'please',\n",
       " 'maybe',\n",
       " 'take',\n",
       " 'I',\n",
       " 'my',\n",
       " 'stop',\n",
       " 'cute',\n",
       " 'is',\n",
       " 'problems',\n",
       " 'buying',\n",
       " 'food',\n",
       " 'licks',\n",
       " 'how',\n",
       " 'love',\n",
       " 'worthless',\n",
       " 'dog',\n",
       " 'ate']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList,listOposts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train: calculating probabilities from word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of documents in each class\n",
    "\n",
    "for every training document:\n",
    "\n",
    "    for each class:\n",
    "    \n",
    "        if a token appears in the document ➞ increment the count for that token\n",
    "        increment the count for tokens\n",
    "    \n",
    "    for each class:\n",
    "            \n",
    "         for each token:\n",
    "            \n",
    "            divide the token count by the total token count to get conditional probabilities\n",
    "    \n",
    "    return conditional probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    #Initialize Probabality\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        #Vector Addition\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    #Element-wise Division\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "# we use log because if there is too much multiplication of small numbers in python. it leads to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainMat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This for loop populates the trainMat list with word vectors\n",
    "for postinDc in listOposts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDc))\n",
    "p0V,p1V,pAb = trainNB0(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the probability of any document being abusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes classify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise multiplication\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word appears more than once in a\n",
    "document, that might convey some sort of information about the document over just\n",
    "the word occurring in the document or not. This approach is known as a bag-of-words\n",
    "model. A bag of words can have multiple occurrences of each word, whereas a set of\n",
    "words can have only one occurrence of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emailText =open(r'C:\\Users\\piush\\Desktop\\Data\\pythonTutorials\\ML in action\\email\\ham\\6.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textParse(), takes a big string and parses out the text into a list of\n",
    "strings. It eliminates anything under two characters long and converts everything to\n",
    "lowercase.\n",
    "\n",
    "spamTest(), automates the naïve Bayes spam classifier.\n",
    "\n",
    "You’ve done only one iteration, but to get a good estimate\n",
    "of our classifier’s true error, you should do this multiple times and take the average\n",
    "error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for anything that isn't a word or number\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "    \n",
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open(r'C:\\Users\\piush\\Desktop\\Data\\pythonTutorials\\ML in action\\email\\spam\\%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open(r'C:\\Users\\piush\\Desktop\\Data\\pythonTutorials\\ML in action\\email\\ham\\%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    #The emails that go into the test set and the training set will be randomly selected.\n",
    "    trainingSet = list(range(50)); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print (\"classification error\",docList[docIndex])\n",
    "    print ('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    #return vocabList,fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['scifinance', 'now', 'automatically', 'generates', 'gpu', 'enabled', 'pricing', 'risk', 'model', 'source', 'code', 'that', 'runs', '300x', 'faster', 'than', 'serial', 'code', 'using', 'new', 'nvidia', 'fermi', 'class', 'tesla', 'series', 'gpu', 'scifinance', 'derivatives', 'pricing', 'and', 'risk', 'model', 'development', 'tool', 'that', 'automatically', 'generates', 'and', 'gpu', 'enabled', 'source', 'code', 'from', 'concise', 'high', 'level', 'model', 'specifications', 'parallel', 'computing', 'cuda', 'programming', 'expertise', 'required', 'scifinance', 'automatic', 'gpu', 'enabled', 'monte', 'carlo', 'pricing', 'model', 'source', 'code', 'generation', 'capabilities', 'have', 'been', 'significantly', 'extended', 'the', 'latest', 'release', 'this', 'includes']\n",
      "classification error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\n",
      "the error rate is:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny = feedparser.parse('http://newyork.craiglist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny['entries']\n",
    "len(ny['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate frequency of occurence\n",
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedFreq[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small percentage of the total\n",
    "words makes up a large portion of the text. The reason for this is that a large percentage\n",
    "of language is redundancy and structural glue. Another common approach is to not just\n",
    "remove the most common words but to also remove this structural glue from a predefined\n",
    "list. This is known as a stop word list, and there are a number of sources of this\n",
    "available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        #Access one feed at a time\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = list(range(2*minLen)); testSet=[]           #create test set\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print ('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny = feedparser.parse('http://newyork.craiglist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craiglist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "vocabList,pSF,pNY = localWords(ny,sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze: displaying locally used words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sort the vectors pSF and pNY and then print out the words from vocabList at\n",
    "the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny,sf):\n",
    "    import operator\n",
    "    vocabList,p0V,p1V=localWords(ny,sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sortedSF:\n",
    "        print (item[0])\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print (\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sortedNY:\n",
    "        print (item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.35\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "would\n",
      "indian\n",
      "open\n",
      "home\n",
      "work\n",
      "lady\n",
      "hate\n",
      "professional\n",
      "coming\n",
      "from\n",
      "what\n",
      "chat\n",
      "more\n",
      "good\n",
      "alone\n",
      "really\n",
      "people\n",
      "shopping\n",
      "working\n",
      "park\n",
      "area\n",
      "wearing\n",
      "hoping\n",
      "give\n",
      "will\n",
      "see\n",
      "married\n",
      "sure\n",
      "funny\n",
      "all\n",
      "office\n",
      "wouldn\n",
      "take\n",
      "going\n",
      "wear\n",
      "feel\n",
      "any\n",
      "read\n",
      "attractive\n",
      "older\n",
      "underwear\n",
      "they\n",
      "hates\n",
      "find\n",
      "poetry\n",
      "interests\n",
      "emails\n",
      "still\n",
      "exchange\n",
      "write\n",
      "age\n",
      "etc\n",
      "basically\n",
      "two\n",
      "here\n",
      "little\n",
      "tell\n",
      "ideas\n",
      "water\n",
      "whether\n",
      "hello\n",
      "poker\n",
      "idea\n",
      "mature\n",
      "laid\n",
      "friend\n",
      "symphony\n",
      "wanted\n",
      "browsing\n",
      "leisure\n",
      "participle\n",
      "nickel\n",
      "evenings\n",
      "years\n",
      "benefits\n",
      "maybe\n",
      "only\n",
      "lol\n",
      "drug\n",
      "sharing\n",
      "sun\n",
      "ads\n",
      "expectations\n",
      "creek\n",
      "wednesday\n",
      "answer\n",
      "bad\n",
      "enforcement\n",
      "wants\n",
      "yet\n",
      "learn\n",
      "already\n",
      "talking\n",
      "well\n",
      "tall\n",
      "know\n",
      "email\n",
      "male\n",
      "down\n",
      "drugs\n",
      "man\n",
      "woman\n",
      "got\n",
      "expert\n",
      "board\n",
      "dinner\n",
      "too\n",
      "semi\n",
      "city\n",
      "lonely\n",
      "hang\n",
      "women\n",
      "word\n",
      "week\n",
      "luck\n",
      "wonder\n",
      "occasion\n",
      "desi\n",
      "discreet\n",
      "boat\n",
      "taking\n",
      "plays\n",
      "try\n",
      "settled\n",
      "games\n",
      "party\n",
      "thhongs\n",
      "movies\n",
      "dime\n",
      "because\n",
      "meet\n",
      "couple\n",
      "while\n",
      "shores\n",
      "others\n",
      "massage\n",
      "other\n",
      "also\n",
      "daily\n",
      "along\n",
      "without\n",
      "basicall\n",
      "cute\n",
      "fran\n",
      "such\n",
      "divorced\n",
      "safer\n",
      "afternoons\n",
      "kno\n",
      "downrig\n",
      "downtown\n",
      "chance\n",
      "therapist\n",
      "had\n",
      "highly\n",
      "shape\n",
      "type\n",
      "issues\n",
      "enthusiast\n",
      "stakes\n",
      "psych\n",
      "them\n",
      "pleasur\n",
      "honest\n",
      "under\n",
      "relax\n",
      "three\n",
      "posting\n",
      "far\n",
      "called\n",
      "practice\n",
      "someth\n",
      "kids\n",
      "informative\n",
      "folks\n",
      "firgure\n",
      "experience\n",
      "through\n",
      "players\n",
      "homeless\n",
      "fascinated\n",
      "east\n",
      "content\n",
      "stop\n",
      "massages\n",
      "caucasian\n",
      "weekend\n",
      "back\n",
      "worldview\n",
      "law\n",
      "vaguely\n",
      "green\n",
      "writer\n",
      "san\n",
      "than\n",
      "gay\n",
      "beautiful\n",
      "business\n",
      "administration\n",
      "tried\n",
      "handsome\n",
      "explode\n",
      "buddy\n",
      "chatting\n",
      "live\n",
      "different\n",
      "sentences\n",
      "guy\n",
      "include\n",
      "school\n",
      "being\n",
      "retired\n",
      "town\n",
      "massager\n",
      "opera\n",
      "join\n",
      "earth\n",
      "together\n",
      "everything\n",
      "entertainment\n",
      "reply\n",
      "talk\n",
      "wears\n",
      "bay\n",
      "walnut\n",
      "started\n",
      "strrings\n",
      "enjoy\n",
      "municipal\n",
      "often\n",
      "shit\n",
      "rub\n",
      "responsibilities\n",
      "get\n",
      "tattooed\n",
      "told\n",
      "big\n",
      "reading\n",
      "both\n",
      "real\n",
      "company\n",
      "one\n",
      "pure\n",
      "forehead\n",
      "friendship\n",
      "anyone\n",
      "bored\n",
      "frightened\n",
      "eating\n",
      "these\n",
      "doing\n",
      "risks\n",
      "cool\n",
      "always\n",
      "today\n",
      "given\n",
      "drink\n",
      "offer\n",
      "probably\n",
      "young\n",
      "clothes\n",
      "having\n",
      "wiccan\n",
      "mind\n",
      "better\n",
      "saturday\n",
      "shares\n",
      "part\n",
      "unlucky\n",
      "relationship\n",
      "intimate\n",
      "witty\n",
      "group\n",
      "hopefully\n",
      "thought\n",
      "dangling\n",
      "teaming\n",
      "routine\n",
      "craigslist\n",
      "involved\n",
      "outwardly\n",
      "silly\n",
      "reach\n",
      "fit\n",
      "around\n",
      "between\n",
      "career\n",
      "notice\n",
      "league\n",
      "else\n",
      "perfect\n",
      "commute\n",
      "joined\n",
      "easily\n",
      "blk\n",
      "sfo\n",
      "incre\n",
      "grab\n",
      "trapped\n",
      "haven\n",
      "kick\n",
      "sociali\n",
      "incredible\n",
      "hospitality\n",
      "madison\n",
      "into\n",
      "personnal\n",
      "where\n",
      "sad\n",
      "quiet\n",
      "race\n",
      "world\n",
      "food\n",
      "when\n",
      "long\n",
      "college\n",
      "wondering\n",
      "did\n",
      "non\n",
      "great\n",
      "pics\n",
      "stem\n",
      "variety\n",
      "pro\n",
      "smoker\n",
      "inspire\n",
      "fellas\n",
      "catches\n",
      "tactile\n",
      "hours\n",
      "anytime\n",
      "oakland\n",
      "gen\n",
      "diablo\n",
      "mail\n",
      "friendly\n",
      "ipad\n",
      "ass\n",
      "nothing\n",
      "drinks\n",
      "person\n",
      "few\n",
      "weed\n",
      "trip\n",
      "str8\n",
      "never\n",
      "snuggle\n",
      "molly\n",
      "nice\n",
      "level\n",
      "cook\n",
      "info\n",
      "over\n",
      "adventure\n",
      "internship\n",
      "interesting\n",
      "late\n",
      "gems\n",
      "hook\n",
      "another\n",
      "curios\n",
      "confident\n",
      "telling\n",
      "pta\n",
      "compact\n",
      "girls\n",
      "foreigners\n",
      "ill\n",
      "420\n",
      "curious\n",
      "title\n",
      "masc\n",
      "hispanic\n",
      "pier\n",
      "concentrate\n",
      "lived\n",
      "new\n",
      "until\n",
      "stats\n",
      "desperate\n",
      "healthy\n",
      "conventions\n",
      "ability\n",
      "anything\n",
      "looks\n",
      "help\n",
      "coast\n",
      "bbw\n",
      "goes\n",
      "much\n",
      "clean\n",
      "serious\n",
      "game\n",
      "response\n",
      "bro\n",
      "unable\n",
      "ten\n",
      "send\n",
      "bounce\n",
      "sent\n",
      "family\n",
      "wanting\n",
      "random\n",
      "movie\n",
      "southern\n",
      "lot\n",
      "sweet\n",
      "skilled\n",
      "total\n",
      "webdesign\n",
      "allergic\n",
      "enough\n",
      "cruise\n",
      "problem\n",
      "guidance\n",
      "teach\n",
      "mentally\n",
      "money\n",
      "japanese\n",
      "unfamiliar\n",
      "white\n",
      "magic\n",
      "plain\n",
      "able\n",
      "smoke\n",
      "expressing\n",
      "melodramatic\n",
      "weird\n",
      "stud\n",
      "reciprocation\n",
      "things\n",
      "stone\n",
      "yes\n",
      "4chan\n",
      "won\n",
      "explain\n",
      "sane\n",
      "indulgence\n",
      "engage\n",
      "eat\n",
      "wash\n",
      "clitoris\n",
      "decisions\n",
      "havnt\n",
      "yrs\n",
      "137\n",
      "something\n",
      "mysterious\n",
      "needs\n",
      "smart\n",
      "beach\n",
      "opinions\n",
      "pic\n",
      "house\n",
      "starting\n",
      "sex\n",
      "chill\n",
      "interested\n",
      "conversation\n",
      "parks\n",
      "very\n",
      "protege\n",
      "was\n",
      "roots\n",
      "show\n",
      "stuck\n",
      "gonna\n",
      "free\n",
      "baked\n",
      "employed\n",
      "weeks\n",
      "come\n",
      "case\n",
      "basketball\n",
      "charge\n",
      "cuddle\n",
      "simple\n",
      "tonight\n",
      "music\n",
      "how\n",
      "desire\n",
      "hands\n",
      "conversing\n",
      "tho\n",
      "making\n",
      "compassionate\n",
      "sexual\n",
      "sea\n",
      "dominant\n",
      "buddies\n",
      "eyes\n",
      "blogs\n",
      "searching\n",
      "express\n",
      "thinking\n",
      "indigenous\n",
      "fancy\n",
      "anime\n",
      "order\n",
      "meeting\n",
      "keeps\n",
      "meal\n",
      "anyway\n",
      "intelligent\n",
      "suffocating\n",
      "become\n",
      "fix\n",
      "explore\n",
      "rare\n",
      "generally\n",
      "mangas\n",
      "encounter\n",
      "legal\n",
      "latino\n",
      "ladies\n",
      "transgender\n",
      "put\n",
      "lil\n",
      "drive\n",
      "completely\n",
      "care\n",
      "then\n",
      "ski\n",
      "aged\n",
      "shoot\n",
      "cooking\n",
      "philosophy\n",
      "her\n",
      "forbidden\n",
      "objectification\n",
      "berkeley\n",
      "porn\n",
      "boyfriend\n",
      "high\n",
      "affectionate\n",
      "partner\n",
      "idc\n",
      "let\n",
      "please\n",
      "hentai\n",
      "kind\n",
      "travel\n",
      "love\n",
      "trained\n",
      "reflexology\n",
      "wherever\n",
      "men\n",
      "internet\n",
      "muse\n",
      "writing\n",
      "whenever\n",
      "secret\n",
      "full\n",
      "hearth\n",
      "swedish\n",
      "place\n",
      "knows\n",
      "next\n",
      "desperately\n",
      "doesnt\n",
      "dishes\n",
      "watch\n",
      "she\n",
      "sit\n",
      "unquenchable\n",
      "life\n",
      "references\n",
      "most\n",
      "plse\n",
      "necessary\n",
      "mails\n",
      "starcraft\n",
      "min\n",
      "philadelphia\n",
      "myself\n",
      "educated\n",
      "reddit\n",
      "pass\n",
      "ask\n",
      "past\n",
      "student\n",
      "chromosome\n",
      "buy\n",
      "literally\n",
      "glutes\n",
      "assistant\n",
      "problems\n",
      "hidden\n",
      "soft\n",
      "somewhat\n",
      "close\n",
      "laptop\n",
      "personality\n",
      "own\n",
      "sexy\n",
      "gathering\n",
      "pretending\n",
      "last\n",
      "against\n",
      "hmu\n",
      "trying\n",
      "everyday\n",
      "respect\n",
      "easy\n",
      "including\n",
      "respectful\n",
      "girl\n",
      "oriented\n",
      "organize\n",
      "playing\n",
      "times\n",
      "orgasm\n",
      "keep\n",
      "normally\n",
      "grew\n",
      "less\n",
      "short\n",
      "prefer\n",
      "nyc\n",
      "pokemon\n",
      "troubled\n",
      "empathe\n",
      "now\n",
      "whatever\n",
      "head\n",
      "regular\n",
      "doesn\n",
      "swap\n",
      "barbie\n",
      "ropes\n",
      "understands\n",
      "bases\n",
      "table\n",
      "legends\n",
      "strong\n",
      "cooked\n",
      "loan\n",
      "story\n",
      "minded\n",
      "yourself\n",
      "ship\n",
      "tend\n",
      "hobo\n",
      "television\n",
      "every\n",
      "morals\n",
      "transplant\n",
      "listen\n",
      "30s\n",
      "seek\n",
      "seeking\n",
      "profesionnal\n",
      "living\n",
      "feet\n",
      "introvert\n",
      "areas\n",
      "aren\n",
      "bars\n",
      "occasional\n",
      "right\n",
      "been\n",
      "car\n",
      "its\n",
      "hey\n",
      "video\n",
      "sub\n",
      "restless\n",
      "gasp\n",
      "bottom\n",
      "attached\n",
      "job\n",
      "day\n",
      "journey\n",
      "off\n",
      "dont\n",
      "pleasure\n",
      "readily\n",
      "note\n",
      "depressed\n",
      "doujins\n",
      "old\n",
      "responses\n",
      "why\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "life\n",
      "things\n",
      "yes\n",
      "travel\n",
      "men\n",
      "talk\n",
      "problems\n",
      "area\n",
      "food\n",
      "non\n",
      "pics\n",
      "woman\n",
      "person\n",
      "cook\n",
      "new\n",
      "meet\n",
      "send\n",
      "family\n",
      "engage\n",
      "hands\n",
      "sexual\n",
      "dominant\n",
      "watch\n",
      "myself\n",
      "educated\n",
      "buy\n",
      "personality\n",
      "work\n",
      "trying\n",
      "get\n",
      "etc\n",
      "yourself\n",
      "relationship\n",
      "job\n",
      "perfect\n",
      "joined\n",
      "easily\n",
      "blk\n",
      "incre\n",
      "only\n",
      "lol\n",
      "grab\n",
      "kick\n",
      "hospitality\n",
      "sad\n",
      "quiet\n",
      "race\n",
      "college\n",
      "wondering\n",
      "from\n",
      "what\n",
      "did\n",
      "well\n",
      "pro\n",
      "smoker\n",
      "down\n",
      "fellas\n",
      "tactile\n",
      "hoping\n",
      "man\n",
      "friendly\n",
      "ass\n",
      "give\n",
      "nothing\n",
      "drinks\n",
      "weed\n",
      "str8\n",
      "never\n",
      "snuggle\n",
      "molly\n",
      "nice\n",
      "too\n",
      "over\n",
      "interesting\n",
      "will\n",
      "gems\n",
      "open\n",
      "curios\n",
      "week\n",
      "telling\n",
      "compact\n",
      "420\n",
      "curious\n",
      "masc\n",
      "hispanic\n",
      "would\n",
      "concentrate\n",
      "party\n",
      "desperate\n",
      "healthy\n",
      "ability\n",
      "funny\n",
      "looks\n",
      "help\n",
      "bbw\n",
      "much\n",
      "take\n",
      "going\n",
      "clean\n",
      "while\n",
      "unable\n",
      "other\n",
      "wanting\n",
      "movie\n",
      "lot\n",
      "skilled\n",
      "total\n",
      "money\n",
      "smoke\n",
      "issues\n",
      "won\n",
      "sane\n",
      "wash\n",
      "yrs\n",
      "137\n",
      "something\n",
      "pic\n",
      "house\n",
      "starting\n",
      "sex\n",
      "stuck\n",
      "free\n",
      "baked\n",
      "employed\n",
      "weeks\n",
      "attractive\n",
      "case\n",
      "charge\n",
      "cuddle\n",
      "conversing\n",
      "sea\n",
      "buddies\n",
      "searching\n",
      "keeps\n",
      "chat\n",
      "intelligent\n",
      "become\n",
      "fix\n",
      "older\n",
      "explore\n",
      "back\n",
      "they\n",
      "ladies\n",
      "lil\n",
      "ski\n",
      "shoot\n",
      "affectionate\n",
      "kind\n",
      "love\n",
      "trained\n",
      "reflexology\n",
      "secret\n",
      "full\n",
      "swedish\n",
      "knows\n",
      "next\n",
      "being\n",
      "dishes\n",
      "good\n",
      "references\n",
      "min\n",
      "philadelphia\n",
      "ask\n",
      "reply\n",
      "glutes\n",
      "hidden\n",
      "enjoy\n",
      "close\n",
      "own\n",
      "respect\n",
      "including\n",
      "respectful\n",
      "both\n",
      "oriented\n",
      "organize\n",
      "one\n",
      "friendship\n",
      "anyone\n",
      "bored\n",
      "normally\n",
      "prefer\n",
      "doing\n",
      "nyc\n",
      "troubled\n",
      "regular\n",
      "always\n",
      "swap\n",
      "people\n",
      "bases\n",
      "table\n",
      "loan\n",
      "minded\n",
      "ship\n",
      "mind\n",
      "two\n",
      "every\n",
      "here\n",
      "seek\n",
      "feet\n",
      "introvert\n",
      "bars\n",
      "been\n",
      "car\n",
      "its\n",
      "hey\n",
      "sub\n",
      "restless\n",
      "day\n",
      "dont\n",
      "hello\n",
      "depressed\n",
      "old\n",
      "responses\n",
      "around\n",
      "poker\n",
      "idea\n",
      "mature\n",
      "laid\n",
      "between\n",
      "career\n",
      "friend\n",
      "notice\n",
      "league\n",
      "symphony\n",
      "else\n",
      "commute\n",
      "wanted\n",
      "browsing\n",
      "leisure\n",
      "working\n",
      "participle\n",
      "nickel\n",
      "evenings\n",
      "coming\n",
      "years\n",
      "benefits\n",
      "sfo\n",
      "park\n",
      "maybe\n",
      "trapped\n",
      "drug\n",
      "haven\n",
      "sociali\n",
      "sharing\n",
      "incredible\n",
      "sun\n",
      "madison\n",
      "ads\n",
      "wearing\n",
      "into\n",
      "personnal\n",
      "expectations\n",
      "creek\n",
      "where\n",
      "wednesday\n",
      "world\n",
      "answer\n",
      "bad\n",
      "enforcement\n",
      "wants\n",
      "when\n",
      "yet\n",
      "learn\n",
      "already\n",
      "long\n",
      "talking\n",
      "great\n",
      "stem\n",
      "tall\n",
      "variety\n",
      "know\n",
      "inspire\n",
      "email\n",
      "male\n",
      "drugs\n",
      "catches\n",
      "hours\n",
      "anytime\n",
      "oakland\n",
      "gen\n",
      "diablo\n",
      "mail\n",
      "ipad\n",
      "got\n",
      "few\n",
      "trip\n",
      "expert\n",
      "level\n",
      "board\n",
      "dinner\n",
      "info\n",
      "adventure\n",
      "semi\n",
      "internship\n",
      "city\n",
      "late\n",
      "lonely\n",
      "hang\n",
      "hook\n",
      "another\n",
      "see\n",
      "married\n",
      "sure\n",
      "women\n",
      "confident\n",
      "word\n",
      "pta\n",
      "girls\n",
      "luck\n",
      "foreigners\n",
      "wonder\n",
      "ill\n",
      "occasion\n",
      "desi\n",
      "discreet\n",
      "boat\n",
      "taking\n",
      "title\n",
      "plays\n",
      "try\n",
      "pier\n",
      "settled\n",
      "games\n",
      "lived\n",
      "until\n",
      "thhongs\n",
      "stats\n",
      "conventions\n",
      "movies\n",
      "anything\n",
      "all\n",
      "office\n",
      "coast\n",
      "dime\n",
      "wouldn\n",
      "because\n",
      "goes\n",
      "couple\n",
      "shores\n",
      "serious\n",
      "game\n",
      "response\n",
      "bro\n",
      "others\n",
      "ten\n",
      "massage\n",
      "bounce\n",
      "sent\n",
      "also\n",
      "random\n",
      "southern\n",
      "wear\n",
      "daily\n",
      "along\n",
      "sweet\n",
      "without\n",
      "basicall\n",
      "cute\n",
      "fran\n",
      "webdesign\n",
      "allergic\n",
      "enough\n",
      "such\n",
      "cruise\n",
      "problem\n",
      "guidance\n",
      "divorced\n",
      "safer\n",
      "afternoons\n",
      "teach\n",
      "mentally\n",
      "kno\n",
      "japanese\n",
      "downrig\n",
      "unfamiliar\n",
      "white\n",
      "magic\n",
      "downtown\n",
      "plain\n",
      "able\n",
      "chance\n",
      "therapist\n",
      "had\n",
      "expressing\n",
      "highly\n",
      "shape\n",
      "melodramatic\n",
      "weird\n",
      "stud\n",
      "reciprocation\n",
      "type\n",
      "enthusiast\n",
      "stone\n",
      "4chan\n",
      "feel\n",
      "stakes\n",
      "explain\n",
      "indulgence\n",
      "eat\n",
      "psych\n",
      "clitoris\n",
      "decisions\n",
      "them\n",
      "havnt\n",
      "any\n",
      "pleasur\n",
      "mysterious\n",
      "needs\n",
      "smart\n",
      "beach\n",
      "opinions\n",
      "chill\n",
      "honest\n",
      "interested\n",
      "under\n",
      "relax\n",
      "three\n",
      "conversation\n",
      "posting\n",
      "home\n",
      "parks\n",
      "very\n",
      "far\n",
      "protege\n",
      "was\n",
      "roots\n",
      "show\n",
      "read\n",
      "called\n",
      "practice\n",
      "someth\n",
      "kids\n",
      "gonna\n",
      "informative\n",
      "come\n",
      "indian\n",
      "basketball\n",
      "folks\n",
      "firgure\n",
      "simple\n",
      "tonight\n",
      "music\n",
      "how\n",
      "desire\n",
      "experience\n",
      "tho\n",
      "through\n",
      "making\n",
      "players\n",
      "homeless\n",
      "compassionate\n",
      "fascinated\n",
      "east\n",
      "eyes\n",
      "blogs\n",
      "express\n",
      "thinking\n",
      "indigenous\n",
      "fancy\n",
      "anime\n",
      "order\n",
      "meeting\n",
      "meal\n",
      "anyway\n",
      "content\n",
      "suffocating\n",
      "stop\n",
      "rare\n",
      "generally\n",
      "mangas\n",
      "encounter\n",
      "legal\n",
      "underwear\n",
      "massages\n",
      "more\n",
      "caucasian\n",
      "weekend\n",
      "hates\n",
      "worldview\n",
      "law\n",
      "vaguely\n",
      "latino\n",
      "transgender\n",
      "put\n",
      "find\n",
      "green\n",
      "writer\n",
      "drive\n",
      "completely\n",
      "san\n",
      "care\n",
      "poetry\n",
      "then\n",
      "than\n",
      "aged\n",
      "gay\n",
      "cooking\n",
      "philosophy\n",
      "beautiful\n",
      "business\n",
      "her\n",
      "forbidden\n",
      "objectification\n",
      "berkeley\n",
      "porn\n",
      "boyfriend\n",
      "high\n",
      "administration\n",
      "tried\n",
      "handsome\n",
      "explode\n",
      "buddy\n",
      "chatting\n",
      "partner\n",
      "idc\n",
      "let\n",
      "please\n",
      "hentai\n",
      "live\n",
      "different\n",
      "sentences\n",
      "guy\n",
      "wherever\n",
      "interests\n",
      "internet\n",
      "muse\n",
      "writing\n",
      "whenever\n",
      "hearth\n",
      "place\n",
      "include\n",
      "emails\n",
      "desperately\n",
      "still\n",
      "school\n",
      "doesnt\n",
      "retired\n",
      "town\n",
      "she\n",
      "sit\n",
      "massager\n",
      "unquenchable\n",
      "opera\n",
      "most\n",
      "plse\n",
      "necessary\n",
      "join\n",
      "mails\n",
      "earth\n",
      "together\n",
      "exchange\n",
      "starcraft\n",
      "everything\n",
      "reddit\n",
      "pass\n",
      "entertainment\n",
      "past\n",
      "wears\n",
      "student\n",
      "chromosome\n",
      "bay\n",
      "literally\n",
      "assistant\n",
      "walnut\n",
      "started\n",
      "strrings\n",
      "soft\n",
      "municipal\n",
      "somewhat\n",
      "often\n",
      "laptop\n",
      "shit\n",
      "sexy\n",
      "rub\n",
      "gathering\n",
      "pretending\n",
      "last\n",
      "against\n",
      "hmu\n",
      "responsibilities\n",
      "tattooed\n",
      "everyday\n",
      "told\n",
      "easy\n",
      "big\n",
      "girl\n",
      "alone\n",
      "reading\n",
      "real\n",
      "company\n",
      "playing\n",
      "times\n",
      "pure\n",
      "orgasm\n",
      "forehead\n",
      "really\n",
      "frightened\n",
      "keep\n",
      "write\n",
      "age\n",
      "grew\n",
      "less\n",
      "short\n",
      "eating\n",
      "these\n",
      "basically\n",
      "pokemon\n",
      "lady\n",
      "risks\n",
      "empathe\n",
      "now\n",
      "whatever\n",
      "cool\n",
      "head\n",
      "doesn\n",
      "today\n",
      "barbie\n",
      "ropes\n",
      "understands\n",
      "given\n",
      "drink\n",
      "offer\n",
      "probably\n",
      "legends\n",
      "strong\n",
      "cooked\n",
      "young\n",
      "clothes\n",
      "having\n",
      "story\n",
      "wiccan\n",
      "tend\n",
      "hobo\n",
      "better\n",
      "television\n",
      "saturday\n",
      "morals\n",
      "transplant\n",
      "listen\n",
      "shares\n",
      "30s\n",
      "part\n",
      "little\n",
      "seeking\n",
      "unlucky\n",
      "intimate\n",
      "witty\n",
      "group\n",
      "profesionnal\n",
      "hate\n",
      "hopefully\n",
      "living\n",
      "thought\n",
      "dangling\n",
      "areas\n",
      "teaming\n",
      "aren\n",
      "occasional\n",
      "right\n",
      "tell\n",
      "ideas\n",
      "routine\n",
      "water\n",
      "shopping\n",
      "whether\n",
      "craigslist\n",
      "video\n",
      "gasp\n",
      "bottom\n",
      "attached\n",
      "journey\n",
      "involved\n",
      "off\n",
      "outwardly\n",
      "pleasure\n",
      "readily\n",
      "note\n",
      "professional\n",
      "silly\n",
      "doujins\n",
      "reach\n",
      "fit\n",
      "why\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny,sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note: a lot of stop words\n",
    "appear in the output. It would be interesting to see how things would change if\n",
    "you removed the fixed stop words. Classification error will also\n",
    "go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underflow is one problem that can be addressed\n",
    "by using the logarithm of probabilities in your calculations. The bag-of-words model is\n",
    "an improvement on the set-of-words model when approaching document classification.\n",
    "There are a number of other improvements, such as removing stop words, and\n",
    "you can spend a long time optimizing a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
