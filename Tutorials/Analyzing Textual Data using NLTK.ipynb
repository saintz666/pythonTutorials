{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plain text is the most predominant form of data available today. Text analysis applies analysis of word frequency distributions, pattern recognition, tagging, link and association analysis, sentiment analysis, and visualization. We will analyze text with the Python Natural Language Toolkit (NLTK) library. NLTK comes with a collection of sample texts called corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is a Python API for the analysis of texts written in natural languages, such as\n",
    "English. NLTK was created in 2001 and was originally intended as a teaching tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk version 3.2.1\n",
      "nltk.app DESCRIPTION chartparser: Chart Parser chunkparser: Regular-Expression Chunk Parser collocations: Find collocations in text concordance: Part\n",
      "nltk.ccg DESCRIPTION For more information see nltk/doc/contrib/ccg/ccg.pdf PACKAGE CONTENTS api chart combinator lexicon logic DATA BackwardApplicati\n",
      "nltk.chat DESCRIPTION A class for simple chatbots. These perform simple pattern matching on sentences typed by users, and respond with automatically g\n",
      "nltk.chunk DESCRIPTION Classes and interfaces for identifying non-overlapping linguistic groups (such as base noun phrases) in unrestricted text. This \n",
      "nltk.classify DESCRIPTION Classes and interfaces for labeling tokens with category labels (or \"class labels\"). Typically, labels are represented with stri\n",
      "nltk.cluster DESCRIPTION This module contains a number of basic clustering algorithms. Clustering describes the task of discovering groups of similar ite\n",
      "nltk.corpus \n",
      "nltk.draw DESCRIPTION # Natural Language Toolkit: graphical representations package # # Copyright (C) 2001-2016 NLTK Project # Author: Edward Loper <e\n",
      "nltk.inference \n",
      "nltk.metrics DESCRIPTION # Natural Language Toolkit: Translation metrics # # Copyright (C) 2001-2016 NLTK Project # Author: Will Zhang <wilzzha@gmail.com\n",
      "nltk.misc DESCRIPTION # Natural Language Toolkit: Miscellaneous modules # # Copyright (C) 2001-2016 NLTK Project # Author: Steven Bird <stevenbird1@gm\n",
      "nltk.parse DESCRIPTION Classes and interfaces for producing tree structures that represent the internal organization of a text. This task is known as \"\n",
      "nltk.sem DESCRIPTION This package contains classes for representing semantic structure in formulas of first-order logic and for evaluating such formu\n",
      "nltk.sentiment \n",
      "nltk.stem DESCRIPTION Interfaces used to remove morphological affixes from words, leaving only the word stem. Stemming algorithms aim to remove those \n",
      "nltk.tag DESCRIPTION This package contains classes and interfaces for part-of-speech tagging, or simply \"tagging\". A \"tag\" is a case-sensitive string\n",
      "nltk.tbl DESCRIPTION A general purpose package for Transformation Based Learning, currently used by nltk.tag.BrillTagger. PACKAGE CONTENTS api demo e\n",
      "nltk.test DESCRIPTION Unit tests for the NLTK modules. These tests are intended to ensure that source code changes don't accidentally introduce bugs. \n",
      "nltk.tokenize DESCRIPTION Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a s\n",
      "nltk.translate DESCRIPTION Experimental features for machine translation. These interfaces are prone to change. PACKAGE CONTENTS api bleu_score gale_church\n",
      "nltk.twitter DESCRIPTION This package contains classes for retrieving Tweet documents using the Twitter API. PACKAGE CONTENTS api common twitter_demo twi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pkgutil as pu\n",
    "import pydoc\n",
    "import nltk\n",
    "\n",
    "print (\"nltk version\", nltk.__version__)\n",
    "\n",
    "def clean(astr):\n",
    "   s = astr\n",
    "   # remove multiple spaces\n",
    "   s = ' '.join(s.split())\n",
    "   s = s.replace('=','')\n",
    "\n",
    "   return s\n",
    "\n",
    "def print_desc(prefix, pkg_path):\n",
    "   for pkg in pu.iter_modules(path=pkg_path):\n",
    "      name = prefix + \".\" + pkg[1]\n",
    "\n",
    "      if pkg[2] == True:\n",
    "         try:\n",
    "            docstr = pydoc.plain(pydoc.render_doc(name))\n",
    "            docstr = clean(docstr)\n",
    "            start = docstr.find(\"DESCRIPTION\")\n",
    "            docstr = docstr[start: start + 140]\n",
    "            print (name, docstr)\n",
    "         except:\n",
    "            continue\n",
    "\n",
    "print_desc(\"nltk\", nltk.__path__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to download the NLTK corpora.\n",
    "\n",
    "Go to python\n",
    "\n",
    ">import nltk\n",
    "\n",
    ">nltk.download()\n",
    "\n",
    "A GUI application should appear, where you can specify a destination and what\n",
    "to download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out stopwords, names,and numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a common requirement in text analysis to get rid of stopwords (common words\n",
    "with low information value). NLTK has a stopwords corpora for a number of\n",
    "languages. Load the English stopwords corpus and print some of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words ['while', 'he', 'that', 'both', 'ourselves', 'won', 'been', 'about']\n"
     ]
    }
   ],
   "source": [
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "print (\"Stop words\",list(sw)[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all the words in this corpus are in lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gutenberg project is a digital library of\n",
    "books mostly with expired copyright, which are available for free on the Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg files ['milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "gb = nltk.corpus.gutenberg\n",
    "print (\"Gutenberg files\",gb.fileids()[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the first couple of sentences from the milton-paradise.txt file that we\n",
    "will filter later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered [['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', '1667', ']'], ['Book', 'I']]\n"
     ]
    }
   ],
   "source": [
    "text_sent = gb.sents(\"milton-paradise.txt\")[:2]\n",
    "print (\"Unfiltered\", text_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, filter out the stopwords as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered ['[', 'Paradise', 'Lost', 'John', 'Milton', '1667', ']']\n",
      "Filtered ['Book']\n"
     ]
    }
   ],
   "source": [
    "for sent in text_sent:\n",
    "    filtered = [w for w in sent if w.lower() not in sw]\n",
    "    print (\"Filtered\", filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare with the previous snippet, we notice that the word by has been\n",
    "filtered out as it was found in the stopwords corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to remove\n",
    "numbers and names too. We can remove words based on Part of Speech (POS) tags.\n",
    "In this tagging scheme, numbers correspond to the Cardinal Number (CD) tag.\n",
    "Names correspond to the proper noun singular (NNP) tag. Tagging is an inexact\n",
    "process based on heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged [('[', 'JJ'), ('Paradise', 'NNP'), ('Lost', 'NNP'), ('John', 'NNP'), ('Milton', 'NNP'), ('1667', 'CD'), (']', 'NN')]\n",
      "Tagged [('Book', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Tag the filtered text with the pos_tag() function:\n",
    "for sent in text_sent:\n",
    "    filtered = [w for w in sent if w.lower() not in sw]\n",
    "    tagged = nltk.pos_tag(filtered)\n",
    "    print (\"Tagged\", tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The pos_tag() function returns a list of tuples, where the second element in each\n",
    "tuple is the tag. As you can see, some of the words are tagged as NNP, although they\n",
    "probably shouldn't be. The heuristic here is to tag words as NNP if the first character\n",
    "of a word is uppercase. If we set all the words to be lowercase, we will get a different\n",
    "result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', ']']\n",
      "['Book']\n"
     ]
    }
   ],
   "source": [
    "for sent in text_sent:\n",
    "    filtered = [w for w in sent if w.lower() not in sw]\n",
    "    tagged = nltk.pos_tag(filtered)\n",
    "    words= []\n",
    "\n",
    "    for word in tagged:\n",
    "        if word[1] != 'NNP' and word[1] != 'CD':\n",
    "           words.append(word[0]) \n",
    "\n",
    "    print (words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In the bag-of-words model, \n",
    "we create from a document a bag containing words\n",
    "found in the document. In this model, we don't care about the word order. For each\n",
    "word in the document, we count the number of occurrences. With these word counts,\n",
    "we can do statistical analysis, for instance, to identify spam in e-mail messages.\n",
    "\n",
    "If we have a group of documents, we can view each unique word in the corpus as a\n",
    "feature; here, \"feature\" means parameter or variable. Using all the word counts, we\n",
    "can build a feature vector for each document; \"vector\" is used here in the mathematical\n",
    "sense. If a word is present in the corpus but not in the document, the value of this\n",
    "feature will be 0. Surprisingly, NLTK doesn't have a handy utility currently to create a\n",
    "feature vector. However, the machine learning Python library, scikit-learn, does have\n",
    "a CountVectorizer class that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector [[ 1  0  1 ..., 14  0  1]\n",
      " [ 0  1  0 ...,  1  1  0]]\n",
      "Features ['1599', '1603', 'abhominably', 'abhorred', 'abide']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "gb = nltk.corpus.gutenberg\n",
    "hamlet = gb.raw(\"shakespeare-hamlet.txt\")\n",
    "macbeth = gb.raw(\"shakespeare-macbeth.txt\")\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "print (\"Feature vector\", cv.fit_transform([hamlet, macbeth]).toarray())\n",
    "print (\"Features\", cv.get_feature_names()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK FreqDist class encapsulates a dictionary of words and counts for a\n",
    "given list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ['william', 'amaze', 'signifies', 'julius', 'priests']\n",
      "Counts [1, 1, 1, 1, 2]\n",
      "Max ['c', 'a', 'e', 's', 'a', 'r']\n",
      "Count 0\n",
      "Bigrams [('dangerous', 'flourish'), ('meanes', 'whereof'), ('bad', 'soules'), ('pleasure', 'portia'), ('rabblement', 'howted')]\n",
      "Counts [1, 1, 1, 1, 1]\n",
      "Bigram Max ['let', 'vs']\n",
      "Bigram count 16\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "gb = nltk.corpus.gutenberg\n",
    "words = gb.words(\"shakespeare-caesar.txt\")\n",
    "\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "filtered = [w.lower() for w in words if w.lower() not in sw and w.lower() not in punctuation]\n",
    "\n",
    "fd = nltk.FreqDist(filtered)\n",
    "print (\"Words\", list(fd.keys())[:5])\n",
    "print (\"Counts\", list(fd.values())[:5])\n",
    "print (\"Max\", list(fd.max()))\n",
    "print (\"Count\", fd['d'])\n",
    "\n",
    "fd = nltk.FreqDist(nltk.bigrams(filtered))\n",
    "print (\"Bigrams\", list(fd.keys())[:5])\n",
    "print (\"Counts\", list(fd.values())[:5])\n",
    "print (\"Bigram Max\", list(fd.max()))\n",
    "print (\"Bigram count\", fd[('let', 'vs')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word in this list is of course not an English word, so we may need to add the\n",
    "heuristic that words have a minimum of two characters. The NLTK FreqDist class\n",
    "allows dictionary-like access, but it also has convenience methods.\n",
    "\n",
    "The analysis until this point concerned single words, but we can extend the analysis\n",
    "to word pairs and triplets. These are also called bigrams and trigrams. We can find\n",
    "them with the bigrams() and trigrams() functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to classify words as stopwords or punctuation. As a feature, we will use the\n",
    "word length, since stopwords and punctuation tend to be short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', True), ('is', True), ('in', True), ('he', True), ('ambitious', False)]\n",
      "'behold' class False\n",
      "'the' class True\n",
      "Accuracy 0.8521671826625387\n",
      "Most Informative Features\n",
      "                     len = 7               False : True   =     77.8 : 1.0\n",
      "                     len = 6               False : True   =     52.2 : 1.0\n",
      "                     len = 1                True : False  =     51.8 : 1.0\n",
      "                     len = 2                True : False  =     10.9 : 1.0\n",
      "                     len = 5               False : True   =     10.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def word_features(word):\n",
    "   return {'len': len(word)}\n",
    "\n",
    "def isStopword(word):\n",
    "    return word in sw or word in punctuation\n",
    "\n",
    "gb = nltk.corpus.gutenberg\n",
    "words = gb.words(\"shakespeare-caesar.txt\")\n",
    "\n",
    "labeled_words = ([(word.lower(), isStopword(word.lower())) for word in words])\n",
    "random.seed(42)\n",
    "random.shuffle(labeled_words)\n",
    "print (labeled_words[:5])\n",
    "\n",
    "featuresets = [(word_features(n), word) for (n, word) in labeled_words]\n",
    "cutoff = int(.9 * len(featuresets))\n",
    "train_set, test_set = featuresets[:cutoff], featuresets[cutoff:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (\"'behold' class\", classifier.classify(word_features('behold')))\n",
    "print (\"'the' class\", classifier.classify(word_features('the')))\n",
    "\n",
    "print (\"Accuracy\", nltk.classify.accuracy(classifier, test_set))\n",
    "print (classifier.show_most_informative_features(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opinion mining or sentiment analysis is a hot, new research field dedicated to\n",
    "the automatic evaluation of opinions as expressed on social media, product review\n",
    "websites, or other forums. Often, we want to know whether an opinion is positive,\n",
    "neutral, or negative.\n",
    "\n",
    "As such, we can apply any number of classification algorithms. Another\n",
    "approach is to semiautomatically (with some manual editing) compose a list of\n",
    "words with an associated numerical sentiment score (the word \"good\" can have a\n",
    "score of 5 and the word \"bad\" a score of -5). If we have such a list, we can look up all\n",
    "words in a text document and, for example, sum up all the found sentiment scores.\n",
    "The number of classes can be more than three, like a five-star rating scheme.\n",
    "\n",
    "We will apply Naive Bayes classification to the NLTK movie reviews corpus with the\n",
    "goal of classifying movie reviews as either positive or negative. You may consider more elaborate filtering\n",
    "schemes, but keep in mind that excessive filtering may hurt accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Review Words 1583820\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "import string\n",
    "\n",
    "\n",
    "labeled_docs = [(list(movie_reviews.words(fid)), cat)\n",
    "        for cat in movie_reviews.categories()\n",
    "        for fid in movie_reviews.fileids(cat)]\n",
    "random.seed(42)\n",
    "random.shuffle(labeled_docs)\n",
    "\n",
    "review_words = movie_reviews.words()\n",
    "print (\"# Review Words\", len(review_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete corpus has tens of thousands of unique words that we can use as\n",
    "features. However, using all these words might be inefficient. Select the top five\n",
    "percent of the most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# After filter 710579\n"
     ]
    }
   ],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word in sw or word in punctuation\n",
    "\n",
    "filtered = [w.lower() for w in review_words if not isStopWord(w.lower())]\n",
    "print (\"# After filter\", len(filtered))\n",
    "\n",
    "words = FreqDist(filtered)\n",
    "N = int(.05 * len(words.keys()))\n",
    "word_features = list(words.keys())[:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, we can extract features using a number of methods including\n",
    "the following:\n",
    "\n",
    "• Check whether the given document has a word or not\n",
    "\n",
    "• Determine the number of occurrences of a word for a given document\n",
    "\n",
    "• Normalize word counts so that the maximum normalized word count will\n",
    "be less than or equal to 1\n",
    "\n",
    "• Take the logarithm of counts plus one (to avoid taking the logarithm of zero)\n",
    "\n",
    "• Combine all the previous points into one metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The following function, which uses raw word counts as a metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_features(doc):\n",
    "    doc_words = FreqDist(w for w in doc if not isStopWord(w))\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['count (%s)' % word] = (doc_words.get(word, 0))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.6\n",
      "Most Informative Features\n",
      "          count (boring) = 2                 neg : pos    =     10.7 : 1.0\n",
      "          count (murphy) = 1                 pos : neg    =      7.6 : 1.0\n",
      "            count (tide) = 1                 pos : neg    =      5.8 : 1.0\n",
      "         count (teaming) = 1                 pos : neg    =      5.8 : 1.0\n",
      "       count (costuming) = 1                 pos : neg    =      5.8 : 1.0\n",
      "        count (angelina) = 1                 neg : pos    =      5.5 : 1.0\n",
      "           count (coats) = 1                 pos : neg    =      5.1 : 1.0\n",
      "          count (winter) = 1                 pos : neg    =      5.1 : 1.0\n",
      "        count (supercop) = 1                 pos : neg    =      5.1 : 1.0\n",
      "             count (ass) = 2                 neg : pos    =      4.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(doc_features(d), c) for (d,c) in labeled_docs]\n",
    "train_set, test_set = featuresets[200:], featuresets[:200]\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print (\"Accuracy\", accuracy(classifier, test_set))\n",
    "\n",
    "print (classifier.show_most_informative_features())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon : 2\n",
      "interrotron : 2\n",
      "bully : 24\n",
      "ringwood : 1\n",
      "bernstein : 10\n",
      "blond : 34\n",
      "tectonic : 7\n",
      "mesa : 1\n",
      "customary : 5\n",
      "burgess : 4\n",
      "gooden : 1\n",
      "coil : 1\n",
      "profusely : 1\n",
      "50 : 59\n",
      "dey : 4\n",
      "neckbraces : 1\n",
      "paraplegic : 1\n",
      "attempt : 263\n",
      "regretting : 1\n",
      "weighs : 7\n",
      "lobotomies : 1\n",
      "therapeutic : 1\n",
      "translators : 1\n",
      "association : 11\n",
      "crooked : 15\n",
      "salma : 23\n",
      "dalmantions : 1\n",
      "semitic : 3\n",
      "colleges : 2\n",
      "rubble : 6\n",
      "hordes : 5\n",
      "uncles : 1\n",
      "lodi : 1\n",
      "lined : 4\n",
      "kilmer : 48\n",
      "jour : 1\n",
      "makers : 58\n",
      "embarrsingly : 1\n",
      "system : 117\n",
      "heroism : 4\n",
      "yeshiva : 1\n",
      "men_ : 1\n",
      "shelled : 1\n",
      "educated : 9\n",
      "subjecting : 4\n",
      "wrinkled : 2\n",
      "acheives : 1\n",
      "ambiguities : 2\n",
      "oprah : 8\n",
      "bimbos : 3\n",
      "ascetic : 2\n",
      "ogres : 2\n",
      "righteousness : 1\n",
      "trilogy : 60\n",
      "obstructed : 1\n",
      "tops : 17\n",
      "tracer : 1\n",
      "nighthawks : 1\n",
      "supply : 22\n",
      "sore : 8\n",
      "inuits : 1\n",
      "dislocation : 1\n",
      "stitch : 1\n",
      "fumbling : 8\n",
      "fobbed : 1\n",
      "36th : 1\n",
      "devlin : 8\n",
      "disgust : 11\n",
      "detective : 127\n",
      "ridiculousy : 1\n",
      "splits : 5\n",
      "sign : 87\n",
      "tiresome : 32\n",
      "dispatching : 2\n",
      "1938 : 3\n",
      "lau : 1\n",
      "rem : 1\n",
      "pseudoerotic : 1\n",
      "roxburgh : 3\n",
      "mozell : 1\n",
      "arye : 3\n",
      "ziembicki : 1\n",
      "revving : 1\n",
      "neat : 32\n",
      "demeans : 1\n",
      "dysart : 1\n",
      "ensnare : 1\n",
      "princeton : 4\n",
      "87 : 4\n",
      "shop : 57\n",
      "python : 15\n",
      "jove : 1\n",
      "hagen : 1\n",
      "camaraderie : 4\n",
      "diminish : 2\n",
      "bernheim : 1\n",
      "smuggles : 2\n",
      "cliffs : 3\n",
      "plusse : 1\n",
      "fledgling : 7\n",
      "technobabble : 2\n",
      "tweaks : 1\n",
      "operate : 11\n",
      "spear : 1\n",
      "talia : 2\n",
      "seductress : 3\n",
      "pearl : 12\n",
      "quarrelling : 1\n",
      "gothic : 29\n",
      "dug : 12\n",
      "forces : 101\n",
      "weebo : 4\n",
      "jarrett : 1\n",
      "honoring : 4\n",
      "illiterate : 8\n",
      "battleship : 3\n",
      "interligator : 1\n",
      "overtones : 15\n",
      "feared : 8\n",
      "snickered : 2\n",
      "scrapbook : 5\n",
      "99 : 17\n",
      "outrage : 10\n",
      "telecommunicative : 1\n",
      "waddling : 1\n",
      "gomer : 1\n",
      "awakes : 3\n",
      "dennison : 1\n",
      "unoffensive : 1\n",
      "thames : 1\n",
      "coat : 19\n",
      "15th : 3\n",
      "axe : 10\n",
      "tierney : 5\n",
      "oncoming : 4\n",
      "mosey : 1\n",
      "wino : 2\n",
      "jivin : 1\n",
      "depress : 1\n",
      "rowena : 1\n",
      "nieces : 1\n",
      "haney : 2\n",
      "churns : 3\n",
      "direct : 77\n",
      "treat : 72\n",
      "collapsed : 2\n",
      "milagro : 1\n",
      "madwoman : 1\n",
      "pay : 166\n",
      "installment : 45\n",
      "professory : 1\n",
      "titty : 3\n",
      "pfieffer : 4\n",
      "heaed : 1\n",
      "keaton : 68\n",
      "promised : 28\n",
      "nosebleeds : 1\n",
      "permissiveness : 1\n",
      "1945 : 3\n",
      "automobiles : 8\n",
      "dropout : 4\n",
      "blaine : 7\n",
      "perverse : 12\n",
      "brooklyn : 19\n",
      "diabolical : 18\n",
      "overdone : 32\n",
      "drifted : 1\n",
      "brainless : 18\n",
      "theresas : 1\n",
      "pent : 4\n",
      "glitter : 6\n",
      "malevolent : 11\n",
      "toiling : 3\n",
      "familiarization : 1\n",
      "filmcritic : 1\n",
      "videos : 26\n",
      "palotti : 1\n",
      "whitmore : 1\n",
      "beg : 6\n",
      "incompletely : 1\n",
      "modulated : 2\n",
      "englishmen : 2\n",
      "pierre : 17\n",
      "kull : 1\n",
      "shortcut : 2\n",
      "diminuitive : 1\n",
      "pitfall : 1\n",
      "depictions : 5\n",
      "fergus : 4\n",
      "seediness : 1\n",
      "disclosed : 4\n",
      "stoop : 4\n",
      "contents : 7\n",
      "filled : 179\n",
      "ballot : 1\n",
      "varela : 1\n",
      "officials : 14\n",
      "counseling : 2\n",
      "proceeder : 1\n",
      "parlays : 1\n",
      "urkel : 2\n",
      "squirmy : 1\n",
      "implicates : 1\n",
      "meditations : 1\n",
      "mensch : 1\n",
      "road : 127\n",
      "helming : 5\n",
      "legitimacy : 3\n",
      "learnt : 1\n",
      "siouxsie : 2\n",
      "popped : 7\n",
      "elected : 4\n",
      "gregorio : 2\n",
      "fight : 323\n",
      "aberration : 2\n",
      "minefield : 1\n",
      "signifiers : 1\n",
      "maryam : 1\n",
      "unique : 115\n",
      "metered : 1\n",
      "eckhart : 9\n",
      "specifics : 10\n",
      "lo : 11\n",
      "blockade : 9\n",
      "sassiness : 1\n",
      "informed : 13\n",
      "whines : 6\n",
      "kinship : 5\n",
      "escapade : 2\n",
      "destroying : 30\n",
      "scolding : 1\n",
      "breathlessly : 3\n",
      "precious : 28\n",
      "uncover : 17\n",
      "choudhury : 1\n",
      "_american_beauty_ : 1\n",
      "flounder : 5\n",
      "ucla : 2\n",
      "jeered : 1\n",
      "bar : 74\n",
      "schreiber : 17\n",
      "typify : 1\n",
      "revitalise : 1\n",
      "bluescreen : 4\n",
      "chabert : 5\n",
      "abort : 1\n",
      "cinematography : 140\n",
      "fiel : 1\n",
      "unexciting : 11\n",
      "premiere : 21\n",
      "wily : 5\n",
      "visualized : 1\n",
      "slogans : 7\n",
      "stunts : 60\n",
      "mic : 2\n",
      "starphoenix : 1\n",
      "tummy : 1\n",
      "exploited : 6\n",
      "juxtaposes : 1\n",
      "overpraising : 1\n",
      "priscilla : 4\n",
      "remand : 1\n",
      "ambition : 21\n",
      "liverpool : 2\n",
      "lad : 11\n",
      "shown : 131\n",
      "gies : 1\n",
      "shakesperean : 1\n",
      "conjunction : 2\n",
      "toughness : 2\n",
      "self : 309\n",
      "brook : 2\n",
      "wish : 134\n",
      "interested : 123\n",
      "sensitive : 32\n",
      "tiriel : 1\n",
      "lestercorp : 1\n",
      "dunn : 11\n",
      "anchorman : 1\n",
      "gimmicks : 8\n",
      "stoner : 7\n",
      "renegades : 1\n",
      "objectionable : 2\n",
      "prerecorded : 1\n",
      "perps : 1\n",
      "brighten : 4\n",
      "illegitimate : 6\n",
      "disneyland : 3\n",
      "compiled : 2\n",
      "indicates : 1\n",
      "genzel : 1\n",
      "morphs : 1\n",
      "scholl : 2\n",
      "clinton : 18\n",
      "firefighters : 3\n",
      "reassured : 1\n",
      "v : 27\n",
      "intellectual : 47\n",
      "retrieval : 1\n",
      "stall : 11\n",
      "abilities : 47\n",
      "solidify : 1\n",
      "stimulates : 1\n",
      "utters : 10\n",
      "purr : 3\n",
      "notoriously : 6\n",
      "63 : 2\n",
      "misery : 23\n",
      "bumps : 12\n",
      "vllainous : 1\n",
      "grape : 9\n",
      "looooooong : 1\n",
      "ornithologist : 1\n",
      "1968 : 19\n",
      "worldly : 9\n",
      "darkness : 51\n",
      "bothers : 10\n",
      "descended : 3\n",
      "douriff : 3\n",
      "chapelle : 3\n",
      "modem : 3\n",
      "belongs : 32\n",
      "brashness : 3\n",
      "kumble : 7\n",
      "uncomfortable : 35\n",
      "jansen : 3\n",
      "directional : 1\n",
      "duck : 18\n",
      "eaten : 9\n",
      "rayden : 5\n",
      "dimensional : 92\n",
      "disliking : 5\n",
      "comment : 28\n",
      "contradiction : 7\n",
      "maxi : 1\n",
      "fatal : 39\n",
      "_breakfast_of_champions_ : 2\n",
      "football : 72\n",
      "dourdan : 1\n",
      "adorable : 24\n",
      "addictive : 2\n",
      "erupts : 6\n",
      "gerrald : 1\n",
      "secluding : 1\n",
      "smears : 2\n",
      "logs : 3\n",
      "cici : 2\n",
      "bouts : 9\n",
      "caldwell : 1\n",
      "showed : 57\n",
      "thank : 38\n",
      "thud : 3\n",
      "crystals : 1\n",
      "mcclements : 1\n",
      "supermasochist : 2\n",
      "mindless : 33\n",
      "alert : 11\n",
      "takers : 2\n",
      "impersonal : 6\n",
      "revert : 1\n",
      "evade : 5\n",
      "repaint : 1\n",
      "bullies : 7\n",
      "revisiting : 5\n",
      "permutation : 1\n",
      "woodstock : 1\n",
      "disintegrate : 3\n",
      "spontaneity : 7\n",
      "awaited : 11\n",
      "bosley : 1\n",
      "combatants : 4\n",
      "reich : 4\n",
      "peddling : 2\n",
      "alleviates : 1\n",
      "mohr : 28\n",
      "denied : 15\n",
      "nipple : 4\n",
      "users : 3\n",
      "harkonnen : 5\n",
      "r : 180\n",
      "doffs : 1\n",
      "vore : 1\n",
      "flamboyance : 1\n",
      "persistent : 2\n",
      "mastery : 11\n",
      "tsi : 1\n",
      "hindu : 5\n",
      "ought : 18\n",
      "36 : 6\n",
      "brewster : 7\n",
      "foregone : 2\n",
      "phillipie : 1\n",
      "screenwriters : 68\n",
      "spaghetti : 6\n",
      "winter : 22\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word in sw or word in punctuation\n",
    "\n",
    "review_words = movie_reviews.words()\n",
    "filtered = [w.lower() for w in review_words if not isStopWord(w.lower())]\n",
    "\n",
    "words = FreqDist(filtered)\n",
    "N = int(.01 * len(words.keys()))\n",
    "tags = list(words.keys())[:N]\n",
    "\n",
    "for tag in tags:\n",
    "    print (tag, ':', words[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 term    tfidf\n",
      "2791      cannibalize  0.03035\n",
      "8737            greys  0.03035\n",
      "19964  superintendent  0.03035\n",
      "14011           ology  0.03035\n",
      "2406          briefer  0.03035\n",
      "matter : 10.1599120411\n",
      "review : 10.1620487127\n",
      "seeing : 10.1937953512\n",
      "jokes : 10.1950100205\n",
      "past : 10.2297321489\n",
      "romantic : 10.2707679481\n",
      "directed : 10.2767754198\n",
      "start : 10.3021271495\n",
      "finally : 10.3151923686\n",
      "video : 10.3567282336\n",
      "despite : 10.3635675871\n",
      "ship : 10.3700292095\n",
      "beautiful : 10.4155676298\n",
      "scream : 10.4219706559\n",
      "sequence : 10.4611040981\n",
      "supposed : 10.4735674889\n",
      "shot : 10.4976115264\n",
      "face : 10.5203272836\n",
      "turn : 10.5353055048\n",
      "lives : 10.5361737181\n",
      "later : 10.5365640805\n",
      "tell : 10.5417039031\n",
      "camera : 10.5807382349\n",
      "works : 10.5848494682\n",
      "children : 10.5921429454\n",
      "live : 10.6586081401\n",
      "daughter : 10.6854088195\n",
      "earth : 10.6855377464\n",
      "mr : 10.7112802669\n",
      "car : 10.7143978876\n",
      "believe : 10.7248097299\n",
      "maybe : 10.7378725085\n",
      "person : 10.7659913761\n",
      "book : 10.7988840881\n",
      "worst : 10.8018088939\n",
      "hand : 10.8157477252\n",
      "named : 10.8178389396\n",
      "game : 10.8638134207\n",
      "fight : 10.865479654\n",
      "use : 10.8842748836\n",
      "used : 10.95539438\n",
      "killer : 11.0003117774\n",
      "certainly : 11.0015254298\n",
      "begins : 11.070728771\n",
      "perfect : 11.0960521972\n",
      "relationship : 11.1024006525\n",
      "said : 11.1080271238\n",
      "nice : 11.1243017843\n",
      "days : 11.1380415072\n",
      "kids : 11.1812766206\n",
      "called : 11.1923605141\n",
      "run : 11.198668832\n",
      "playing : 11.2159822088\n",
      "final : 11.2230438804\n",
      "tries : 11.242823442\n",
      "unfortunately : 11.2957176215\n",
      "group : 11.3268435946\n",
      "comic : 11.3899305602\n",
      "left : 11.4382176225\n",
      "entire : 11.4444375405\n",
      "idea : 11.4617222068\n",
      "based : 11.4871251215\n",
      "head : 11.5161588632\n",
      "wrong : 11.5622895457\n",
      "second : 11.5850462649\n",
      "summer : 11.5866867281\n",
      "shows : 11.6351273669\n",
      "main : 11.6606274549\n",
      "soon : 11.7112361203\n",
      "true : 11.75416436\n",
      "turns : 11.8215760948\n",
      "getting : 11.8742370998\n",
      "human : 11.8998474437\n",
      "problem : 11.9960954877\n",
      "written : 12.0057869039\n",
      "hour : 12.0179335141\n",
      "different : 12.1512504404\n",
      "boy : 12.2021974574\n",
      "performances : 12.2393880411\n",
      "house : 12.2519484855\n",
      "simply : 12.2914399064\n",
      "war : 12.2977309419\n",
      "mind : 12.3247505912\n",
      "small : 12.3276289996\n",
      "especially : 12.3527410342\n",
      "rest : 12.3590502439\n",
      "tv : 12.3684288207\n",
      "lost : 12.3986833421\n",
      "completely : 12.435006614\n",
      "looks : 12.4499538941\n",
      "humor : 12.480694233\n",
      "line : 12.5313814412\n",
      "reason : 12.5494107229\n",
      "dead : 12.5503946542\n",
      "friend : 12.5539321159\n",
      "let : 12.5569787727\n",
      "thought : 12.6499656953\n",
      "stars : 12.6882875344\n",
      "couple : 12.7312558969\n",
      "alien : 12.8118021552\n",
      "moments : 12.8898669826\n",
      "evil : 12.9091800826\n",
      "wants : 12.914918443\n",
      "friends : 12.9713574793\n",
      "night : 12.9720817319\n",
      "mother : 13.069765905\n",
      "given : 13.1635315787\n",
      "ending : 13.2410856063\n",
      "play : 13.2412067174\n",
      "feel : 13.2602950613\n",
      "gives : 13.541687801\n",
      "got : 13.5808093056\n",
      "watching : 13.6331043166\n",
      "death : 13.6381469895\n",
      "looking : 13.7168730578\n",
      "girl : 13.7285522289\n",
      "instead : 13.7745066466\n",
      "probably : 13.8083492831\n",
      "city : 13.8422688176\n",
      "school : 13.8975479866\n",
      "father : 14.0663060599\n",
      "music : 14.0757396474\n",
      "help : 14.1204193189\n",
      "sure : 14.1453524919\n",
      "dialogue : 14.2312512358\n",
      "kind : 14.3998038219\n",
      "black : 14.4474096101\n",
      "actor : 14.5224457411\n",
      "sense : 14.629645347\n",
      "want : 14.7257162322\n",
      "pretty : 14.809903906\n",
      "making : 14.8167340087\n",
      "series : 14.8209855258\n",
      "set : 14.887771685\n",
      "half : 14.8924611627\n",
      "money : 14.9011133704\n",
      "bit : 14.9341306206\n",
      "home : 14.976488818\n",
      "place : 15.0498111242\n",
      "trying : 15.1146219532\n",
      "times : 15.1187451249\n",
      "sex : 15.1662923527\n",
      "american : 15.3596374153\n",
      "hard : 15.4540096522\n",
      "picture : 15.5061977848\n",
      "woman : 15.6397754436\n",
      "hollywood : 15.6506038138\n",
      "horror : 15.6880471355\n",
      "far : 15.7585104444\n",
      "watch : 15.7815502447\n",
      "fun : 15.7987918729\n",
      "special : 15.8948944322\n",
      "course : 15.9318979266\n",
      "away : 15.9443709619\n",
      "takes : 15.9540647703\n",
      "men : 16.0361428825\n",
      "wife : 16.1037258424\n",
      "interesting : 16.10997862\n",
      "screen : 16.330493298\n",
      "goes : 16.3551568626\n",
      "minutes : 16.578557694\n",
      "point : 16.593090771\n",
      "quite : 16.7437034704\n",
      "lot : 16.7552127154\n",
      "comes : 16.9450304746\n",
      "high : 16.963040218\n",
      "day : 17.4698957497\n",
      "young : 17.6189943856\n",
      "come : 17.7106880542\n",
      "plays : 17.8246577908\n",
      "actors : 17.8409765663\n",
      "acting : 18.0563958822\n",
      "effects : 18.1563689723\n",
      "fact : 18.3587776783\n",
      "family : 18.4361098826\n",
      "cast : 18.46225158\n",
      "right : 18.5171190436\n",
      "look : 18.7090742959\n",
      "original : 18.8151315839\n",
      "played : 18.9614606622\n",
      "years : 19.058053712\n",
      "long : 19.0630758363\n",
      "actually : 19.0812398566\n",
      "thing : 19.347561164\n",
      "script : 19.4657918053\n",
      "old : 19.6842512246\n",
      "things : 19.7116860665\n",
      "gets : 19.7898795762\n",
      "think : 19.7997825996\n",
      "role : 19.8297421598\n",
      "performance : 19.991860532\n",
      "better : 20.068454555\n",
      "audience : 20.2290207684\n",
      "going : 20.3848011417\n",
      "year : 20.4044183921\n",
      "seen : 20.7416253413\n",
      "real : 20.7820552523\n",
      "makes : 20.9161012814\n",
      "work : 21.4895954383\n",
      "funny : 22.1848390278\n",
      "world : 22.4300122918\n",
      "end : 22.5007754053\n",
      "comedy : 22.7237230337\n",
      "big : 23.3934376656\n",
      "director : 23.6894592429\n",
      "great : 24.4893642113\n",
      "scenes : 25.2431406494\n",
      "know : 25.2589574906\n",
      "new : 25.4239710833\n",
      "movies : 25.464350873\n",
      "best : 25.7928652794\n",
      "scene : 26.6558182497\n",
      "man : 27.2709864037\n",
      "people : 27.7725778275\n",
      "action : 27.8313114208\n",
      "little : 27.9120138787\n",
      "make : 28.3621148946\n",
      "films : 29.081584588\n",
      "bad : 29.1628251087\n",
      "plot : 29.8099103848\n",
      "really : 30.2110190122\n",
      "life : 30.8440483735\n",
      "characters : 33.2042921544\n",
      "character : 33.7339201709\n",
      "story : 36.6938157838\n",
      "time : 36.7684157009\n",
      "good : 38.9452096228\n",
      "like : 49.0869900518\n",
      "movie : 80.3324929412\n",
      "film : 109.339220501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:42: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import names\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "#Improve filtering by using the isalpha() method and names corpus:\n",
    "all_names = set([name.lower() for name in names.words()])\n",
    "\n",
    "def isStopWord(word):\n",
    "    return (word in sw or word in punctuation) or not word.isalpha() or word in all_names\n",
    "\n",
    "review_words = movie_reviews.words()\n",
    "filtered = [w.lower() for w in review_words if not isStopWord(w.lower())]\n",
    "\n",
    "words = FreqDist(filtered)\n",
    "#Create the list as follows:\n",
    "\n",
    "texts = []\n",
    "\n",
    "for fid in movie_reviews.fileids():\n",
    "    texts.append(\" \".join([w.lower() for w in movie_reviews.words(fid) if not isStopWord(w.lower()) and words[w.lower()] > 1]))\n",
    "#Create the vectorizer; to be safe, let it ignore stopwords:\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "#Create the sparse term-document matrix:\n",
    "matrix = vectorizer.fit_transform(texts)\n",
    "#Sum the tf-idf values for each word and store it in a NumPy array:\n",
    "sums = np.array(matrix.sum(axis=0)).ravel()\n",
    "#Now, create a pandas DataFrame with the word rank weights and sort it:\n",
    "ranks = []\n",
    "\n",
    "for word, val in zip(vectorizer.get_feature_names(), sums):\n",
    "    ranks.append((word, val))\n",
    "\n",
    "df = pd.DataFrame(ranks, columns=[\"term\", \"tfidf\"])\n",
    "df = df.sort(['tfidf'])\n",
    "print (df.head())\n",
    "\n",
    "N = int(.01 * len(df))\n",
    "df = df.tail(N)\n",
    "\n",
    "for term, tfidf in zip(df[\"term\"].values, df[\"tfidf\"].values):\n",
    "    print (term, \":\", tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
