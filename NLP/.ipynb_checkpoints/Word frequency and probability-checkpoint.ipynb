{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I took my medicine</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have not taken my remedy</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No never I will not have it</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was never given my medicine for my stomach ache</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had my medicine</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SENTENCE CLASS\n",
       "0                                 I took my medicine   Yes\n",
       "1                         I have not taken my remedy    No\n",
       "2                        No never I will not have it    No\n",
       "3  I was never given my medicine for my stomach ache    No\n",
       "4                                  I had my medicine   Yes"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in some helpful libraries\n",
    "import nltk # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd # dataframes\n",
    "\n",
    "### Read in the data\n",
    "\n",
    "# read our data into a dataframe\n",
    "texts = pd.read_csv(r\"C:\\Users\\piush\\Desktop\\chatbot\\sentences.csv\")\n",
    "\n",
    "# look at the first few rows\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find out how often each author uses each word\n",
    "A lot of NLP applications rely on counting how often certain words are used. (The fancy term for this is \"word frequency\".) Let's look at the word frequency for each of the authors in our dataset. The NLTK has lots of nice built-in functions and data structures for this that we can make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Split data\n",
    "\n",
    "# split the data by author\n",
    "byAuthor = texts.groupby(\"CLASS\")\n",
    "\n",
    "### Tokenize (split into individual words) our text\n",
    "\n",
    "# word frequency by author\n",
    "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['SENTENCE'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    # split the text into individual tokens    \n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    # calculate the frequency of each token\n",
    "    frequency = nltk.FreqDist(tokens)\n",
    "\n",
    "    # add the frequencies for each author to our dictionary\n",
    "    wordFreqByAuthor[name] = (frequency)\n",
    "    \n",
    "# now we have an dictionary where each entry is the frequency distrobution\n",
    "# of words for a specific author.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at how often each writer uses specific words. Since this is a Halloween competition, how about \"blood\", \"scream\" and \"fear\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medication: No\n",
      "0.020202020202020204\n",
      "medication: Yes\n",
      "0.0189873417721519\n",
      "\n",
      "medicine: No\n",
      "0.06060606060606061\n",
      "medicine: Yes\n",
      "0.056962025316455694\n",
      "\n",
      "remedy: No\n",
      "0.015151515151515152\n",
      "remedy: Yes\n",
      "0.012658227848101266\n"
     ]
    }
   ],
   "source": [
    "# see how often each author says \"blood\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"medication: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('medication'))\n",
    "\n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says \"scream\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"medicine: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('medicine'))\n",
    "    \n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says \"fear\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"remedy: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('remedy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use word frequency to guess which author wrote a sentence\n",
    "The general idea is is that different people tend to use different words more or less often. (I had a beloved college professor that was especially fond of \"gestalt\".) If you're not sure who said something but it has a lot of words one person uses a lot in it, then you might guess that they were the one who wrote it.\n",
    "\n",
    "Let's use this general principle to guess who might have been more likely to write the sentence \"It was a dark and stormy night.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to guess authorship is to use the joint probabilty that each \n",
    "# author used each word in a given sentence.\n",
    "\n",
    "# first, let's start with a test sentence\n",
    "testSentence = \"I want to take my medication.\"\n",
    "\n",
    "# and then lowercase & tokenize our test sentence\n",
    "preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "\n",
    "# create an empy dataframe to put our output in\n",
    "testProbailities = pd.DataFrame(columns = ['response','word','probability'])\n",
    "\n",
    "# For each author...\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # for each word in our test sentence...\n",
    "    for j  in preProcessedTestSentence:\n",
    "        # find out how frequently the author used that word\n",
    "        wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "        # and add a very small amount to every prob. so none of them are 0\n",
    "        smoothedWordFreq = wordFreq + 0.000001\n",
    "        # add the author, word and smoothed freq. to our dataframe\n",
    "        output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['response','word','probability'])\n",
    "        testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "# empty dataframe for the probability that each author wrote the sentence\n",
    "testProbailitiesByAuthor = pd.DataFrame(columns = ['response','jointProbability'])\n",
    "\n",
    "# now let's group the dataframe with our frequency by author\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # get the joint probability that each author wrote each word\n",
    "    oneAuthor = testProbailities.query('response == \"' + i + '\"')\n",
    "    jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "    \n",
    "    # and add that to our dataframe\n",
    "    output = pd.DataFrame([[i, jointProbability]], columns = ['response','jointProbability'])\n",
    "    testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "\n",
    "# and our winner is...\n",
    "testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So based on what we've seen in our training data, it looks like of our three authors, H.P. Lovecraft was the most likely to write the sentence \"It was a dark and stormy night\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
