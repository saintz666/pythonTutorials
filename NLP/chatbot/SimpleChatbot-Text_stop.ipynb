{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import hashlib\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "# use natural language toolkit\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "sno = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\piush\\Desktop\\chatbot\\sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I took my medicine</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have not taken my remedy</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No never I will not have it</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was never given my medicine for my stomach ache</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had my medicine</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SENTENCE CLASS\n",
       "0                                 I took my medicine   Yes\n",
       "1                         I have not taken my remedy    No\n",
       "2                        No never I will not have it    No\n",
       "3  I was never given my medicine for my stomach ache    No\n",
       "4                                  I had my medicine   Yes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VerbCombos = ['VB',\n",
    "              'VBD',\n",
    "              'VBG',\n",
    "              'VBN',\n",
    "              'VBP',\n",
    "              'VBZ',\n",
    "              'WDT',\n",
    "              'WP',\n",
    "              'WP$',\n",
    "              'WRB',\n",
    "              'MD']\n",
    "\n",
    "statementTriples = ['DT-JJ-NN',\n",
    "                   'DT-NN-VBZ',\n",
    "                   'DT-NNP-NNP',\n",
    "                   'IN-DT-NN',\n",
    "                   'IN-NN-NNS',\n",
    "                   'MD-VB-VBN',\n",
    "                   'NNP-IN-NNP',\n",
    "                   'NNP-NNP-NNP',\n",
    "                   'NNP-VBZ-DT',\n",
    "                   'NNP-VBZ-NNP',\n",
    "                   'NNS-IN-DT',\n",
    "                   'VB-VBN-IN',\n",
    "                   'VBZ-DT-JJ']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pass in a list of strings (i.e. PoS types) and the sentence to check PoS types for\n",
    "# check if *Any Pair Combo* of the PoS types list exists in the sentence PoS types\n",
    "# return a count of occurrence\n",
    "def exists_pair_combos(comboCheckList, sentence):\n",
    "    pos = get_pos(sentence)\n",
    "    tag_string = \"-\".join([ i[1] for i in pos ])\n",
    "    combo_list = []\n",
    "    \n",
    "    for pair in itertools.permutations(comboCheckList,2):\n",
    "        if(pair[0] == \"MD\"):  # * Kludge - strip off leading MD *\n",
    "            pair = [\"\",\"\"]\n",
    "        combo_list.append(\"-\".join(pair))\n",
    "    \n",
    "    if any(code in tag_string for code in combo_list):\n",
    "\t    return 1\n",
    "    else:\n",
    "        return 0\n",
    "  \n",
    "# Parts Of Speech\n",
    "def get_pos(sentence):\n",
    "    sentenceParsed = nltk.word_tokenize(sentence)\n",
    "    return(nltk.pos_tag(sentenceParsed))\n",
    "    \n",
    "# Count Q-Marks    \n",
    "def count_qmark(sentence):\n",
    "    return(sentence.count(\"?\") )\n",
    "    \n",
    "# Count a specific POS-Type\n",
    "#VBG = count_POSType(pos,'VBG')\n",
    "def count_POSType(pos, ptype):\n",
    "    count = 0\n",
    "    tags = [ i[1] for i in pos ]\n",
    "    return(tags.count(ptype))\n",
    "    #if ptype in tags:\n",
    "    #    VBG = 1\n",
    "    #return(VBG)\n",
    "    \n",
    "# Does Verb occur before first Noun\n",
    "def exists_vb_before_nn(pos):\n",
    "    pos_tags = [ i[1] for i in pos ]\n",
    "    #Strip the Verbs to all just \"V\"\n",
    "    pos_tags = [ re.sub(r'V.*','V', str) for str in pos_tags ]\n",
    "    #Strip the Nouns to all just \"NN\"\n",
    "    pos_tags = [ re.sub(r'NN.*','NN', str) for str in pos_tags ]\n",
    "    \n",
    "    vi =99\n",
    "    ni =99\n",
    "    mi =99\n",
    "    \n",
    "    #Get first NN index\n",
    "    if \"NN\" in pos_tags:\n",
    "        ni = pos_tags.index(\"NN\")\n",
    "    #Get first V index\n",
    "    if \"V\" in pos_tags:\n",
    "        vi = pos_tags.index(\"V\")\n",
    "    #get Modal Index\n",
    "    if \"MD\" in pos_tags:\n",
    "        mi = pos_tags.index(\"MD\")\n",
    "\n",
    "    if vi < ni or mi < ni :\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "# Stemmed sentence ends in \"NN-NN\"?  \n",
    "def exists_stemmed_end_NN(stemmed):\n",
    "    stemmedEndNN = 0\n",
    "    stemmed_end = get_first_last_tuples(\" \".join(stemmed))[1]\n",
    "    if stemmed_end == \"NN-NN\":\n",
    "        stemmedEndNN = 1\n",
    "    return(stemmedEndNN)\n",
    "\n",
    "# Get a sentence and spit out the POS triples\n",
    "def get_triples(pos):\n",
    "    list_of_triple_strings = []\n",
    "    pos = [ i[1] for i in pos ]  # extract the 2nd element of the POS tuples in list\n",
    "    n = len(pos)\n",
    "    \n",
    "    if n > 2:  # need to have three items\n",
    "        for i in range(0,n-2):\n",
    "            t = \"-\".join(pos[i:i+3]) # pull out 3 list item from counter, convert to string\n",
    "            list_of_triple_strings.append(t)\n",
    "    return list_of_triple_strings \n",
    "\n",
    "def get_first_last_tuples(sentence):\n",
    "    first_last_tuples = []\n",
    "    sentenceParsed = nltk.word_tokenize(sentence)\n",
    "    pos = nltk.pos_tag(sentenceParsed) #Parts Of Speech\n",
    "    pos = [ i[1] for i in pos ]  # extract the 2nd element of the POS tuples in list\n",
    "    \n",
    "    n = len(pos)\n",
    "    first = \"\"\n",
    "    last = \"\"\n",
    "    \n",
    "    if n > 1:  # need to have three items\n",
    "        first = \"-\".join(pos[0:2]) # pull out first 2 list items\n",
    "        last = \"-\".join(pos[-2:]) # pull out last 2 list items\n",
    "    \n",
    "    first_last_tuples = [first, last]\n",
    "    return first_last_tuples\n",
    "        \n",
    "def lemmatize(sentence):    \n",
    "    \"\"\"\n",
    "    pass  in  a sentence as a string, return just core text that has been \"lematised\"\n",
    "    stop words are removed - could effect ability to detect if this is a question or answer\n",
    "    - depends on import lemma = nltk.wordnet.WordNetLemmatizer() and from nltk.corpus import stopwords\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w.lower())  # also set lowercase\n",
    "    lem = []        \n",
    "    for w in filtered_sentence:\n",
    "        lem.append(lemma.lemmatize(w))\n",
    "  \n",
    "    return lem    \n",
    "\n",
    "def stematize(sentence):\n",
    "    \"\"\"\n",
    "    pass  in  a sentence as a string, return just core text stemmed\n",
    "    stop words are removed - could effect ability to detect if this is a question or answer\n",
    "    - depends on import sno = nltk.stem.SnowballStemmer('english') and from nltk.corpus import stopwords\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    stemmed = []        \n",
    "    for w in filtered_sentence:\n",
    "        stemmed.append(sno.stem(w))\n",
    "  \n",
    "    return stemmed    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Count number of question marks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['qMark'] = train.apply(lambda row: count_qmark(row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['wordCount'] = train.apply(lambda row: len(row['SENTENCE'].split()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stop_no = np.setdiff1d(stop, [\"no\",\"not\",\"nor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train['text_without_stopwords'] = (train['SENTENCE'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop) ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['word_tokens'] = train.apply(lambda row: nltk.word_tokenize(row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['stemmed']  =  train.apply(lambda row: stematize(row['text_without_stopwords']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['lemma'] = train.apply(lambda row: lemmatize(row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['stemmedCount'] = train.apply(lambda row: len(row['stemmed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['lemmaCount'] = train.apply(lambda row: len(row['lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemma or Stemm???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['qVerbCombo'] = train.apply(lambda row: exists_pair_combos(VerbCombos,row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"stemmedEndNN\"] = train.apply(lambda row: exists_stemmed_end_NN(row['stemmed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train[\"lemmaEndNN\"] = train.apply(lambda row: exists_stemmed_end_NN(row['lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['pos'] = train.apply(lambda row: get_pos(row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for ptype in [\"VBG\", \"VBZ\", \"NNP\", \"NN\", \"NNS\", \"NNPS\",\"PRP\", \"CD\" ]:\n",
    "        #train['_' + str(ptype)] = train['pos'].apply(lambda row: count_POSType(row,ptype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['verbBeforeNoun'] = train.apply(lambda row: exists_vb_before_nn(row['pos']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['get_triples'] = train.apply(lambda row: get_triples(row['pos']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.corpus import wordnet\n",
    "#train.apply(lambda row: wordnet.synsets(row['SENTENCE']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>stemmedCount</th>\n",
       "      <th>stemmedEndNN</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I took my medicine</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>I took medicine</td>\n",
       "      <td>[i, took, medicin]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[(I, PRP), (took, VBD), (my, PRP$), (medicine,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have not taken my remedy</td>\n",
       "      <td>No</td>\n",
       "      <td>6</td>\n",
       "      <td>I taken remedy</td>\n",
       "      <td>[i, taken, remedi]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[(I, PRP), (have, VBP), (not, RB), (taken, VBN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No never I will not have it</td>\n",
       "      <td>No</td>\n",
       "      <td>7</td>\n",
       "      <td>No never I</td>\n",
       "      <td>[no, never, i]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[(No, DT), (never, RB), (I, PRP), (will, MD), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was never given my medicine for my stomach ache</td>\n",
       "      <td>No</td>\n",
       "      <td>10</td>\n",
       "      <td>I never given medicine stomach ache</td>\n",
       "      <td>[i, never, given, medicin, stomach, ach]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[(I, PRP), (was, VBD), (never, RB), (given, VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had my medicine</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>I medicine</td>\n",
       "      <td>[i, medicin]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[(I, PRP), (had, VBD), (my, PRP$), (medicine, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SENTENCE CLASS  wordCount  \\\n",
       "0                                 I took my medicine   Yes          4   \n",
       "1                         I have not taken my remedy    No          6   \n",
       "2                        No never I will not have it    No          7   \n",
       "3  I was never given my medicine for my stomach ache    No         10   \n",
       "4                                  I had my medicine   Yes          4   \n",
       "\n",
       "                text_without_stopwords  \\\n",
       "0                      I took medicine   \n",
       "1                       I taken remedy   \n",
       "2                           No never I   \n",
       "3  I never given medicine stomach ache   \n",
       "4                           I medicine   \n",
       "\n",
       "                                    stemmed  stemmedCount  stemmedEndNN  \\\n",
       "0                        [i, took, medicin]             3             0   \n",
       "1                        [i, taken, remedi]             3             0   \n",
       "2                            [no, never, i]             3             0   \n",
       "3  [i, never, given, medicin, stomach, ach]             6             1   \n",
       "4                              [i, medicin]             2             1   \n",
       "\n",
       "                                                 pos  \n",
       "0  [(I, PRP), (took, VBD), (my, PRP$), (medicine,...  \n",
       "1  [(I, PRP), (have, VBP), (not, RB), (taken, VBN...  \n",
       "2  [(No, DT), (never, RB), (I, PRP), (will, MD), ...  \n",
       "3  [(I, PRP), (was, VBD), (never, RB), (given, VB...  \n",
       "4  [(I, PRP), (had, VBD), (my, PRP$), (medicine, ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SENTENCE', 'CLASS', 'wordCount', 'text_without_stopwords', 'stemmed',\n",
       "       'stemmedCount', 'stemmedEndNN', 'pos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Rearrange the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train[['SENTENCE', 'wordCount', 'text_without_stopwords', 'stemmed',\n",
    "       'stemmedCount', 'stemmedEndNN', 'pos' , 'CLASS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# class_le = LabelEncoder()\n",
    "# train['text_without_stopwords'] = class_le.fit_transform(train['text_without_stopwords'].values)\n",
    "# train['SENTENCE'] = class_le.fit_transform(train['SENTENCE'].values)\n",
    "# train['stemmed'] = class_le.fit_transform(train['lemma'].values)\n",
    "# train['get_triples'] = class_le.fit_transform(train['get_triples'].values)\n",
    "# train['CLASS'] = class_le.fit_transform(train['CLASS'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SENTENCE', 'wordCount', 'text_without_stopwords', 'stemmed',\n",
       "       'stemmedCount', 'stemmedEndNN', 'pos', 'CLASS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_subset = train[['wordCount', 'stemmed','stemmedCount', 'CLASS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [i, took, medicin]\n",
       "1                          [i, taken, remedi]\n",
       "2                              [no, never, i]\n",
       "3    [i, never, given, medicin, stomach, ach]\n",
       "4                                [i, medicin]\n",
       "Name: stemmed, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset['stemmed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One hot encoding the list (stemmed)\n",
    "\n",
    "https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_subset = train_subset.drop('stemmed', 1).join(train_subset.stemmed.str.join('|').str.get_dummies())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wordCount', 'stemmedCount', 'CLASS', ',', '.', '?', 'accept', 'ach',\n",
       "       'alreadi', 'aspirin', 'back', 'buy', 'cold', 'common', 'cure', 'didnot',\n",
       "       'doctor', 'drug', 'fail', 'get', 'give', 'given', 'herbal', 'howev',\n",
       "       'i', 'know', 'lie', 'medic', 'medicin', 'moment', 'morn', 'nah', 'need',\n",
       "       'never', 'no', 'pain', 'pleas', 'receiv', 'remedi', 'requir', 'respons',\n",
       "       'stomach', 'take', 'taken', 'talk', 'took', 'user', 'want', 'yeah',\n",
       "       'yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the target variable and the training and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_subset['CLASS']\n",
    "X = train_subset.drop(['CLASS'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 49) (43,)\n",
      "(11, 49) (11,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### About cross validation\n",
    "https://medium.com/towards-data-science/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "https://machinelearningmastery.com/evaluate-performance-machine-learning-algorithms-python-using-resampling/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Leave one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "loocv = model_selection.LeaveOneOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.074% (43.823%)\n",
      "[ 1.  1.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.\n",
      "  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  0.  1.  0.  1.  0.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression()\n",
    "score = model_selection.cross_val_score(model_lr, X, y, cv=loocv, scoring='f1_micro')\n",
    "#results = model_selection.cross_val_score(model_lr, X, y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (score.mean()*100.0, score.std()*100.0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.259% (49.135%)\n",
      "[ 1.  0.  1.  0.  1.  0.  0.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1.  1.\n",
      "  1.  1.  0.  1.  0.  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "model_nb = GaussianNB()\n",
    "score = model_selection.cross_val_score(model_nb, X, y, cv=loocv, scoring='f1_micro')\n",
    "#results = model_selection.cross_val_score(model_nb, X, y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (score.mean()*100.0, score.std()*100.0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.259% (49.135%)\n",
      "[ 1.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
      "  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "model_knc = KNeighborsClassifier()\n",
    "#results = model_selection.cross_val_score(model_knc, X, y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (score.mean()*100.0, score.std()*100.0))\n",
    "score = model_selection.cross_val_score(model_knc, X, y, cv=loocv, scoring='f1_micro')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.074% (43.823%)\n",
      "[ 1.  1.  1.  1.  1.  1.  0.  1.  0.  1.  0.  1.  0.  1.  1.  1.  1.  1.\n",
      "  0.  1.  1.  1.  1.  0.  0.  1.  1.  0.  0.  0.  1.  0.  1.  0.  1.  1.\n",
      "  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC()\n",
    "score = model_selection.cross_val_score(model_svc, X, y, cv=loocv, scoring='f1_micro')\n",
    "#results = model_selection.cross_val_score(model_svc, X, y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (score.mean()*100.0, score.std()*100.0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.926% (42.753%)\n",
      "[ 1.  0.  1.  1.  0.  1.  1.  0.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  0.  1.  0.  1.  1.  0.  1.  0.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "model_rfc = RandomForestClassifier()\n",
    "score = model_selection.cross_val_score(model_rfc, X, y, cv=loocv, scoring='f1_micro')\n",
    "results = model_selection.cross_val_score(model_rfc, X, y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (score.mean()*100.0, score.std()*100.0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to script SimpleChatbot.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
