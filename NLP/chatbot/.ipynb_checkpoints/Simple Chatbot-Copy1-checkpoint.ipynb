{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import hashlib\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "# use natural language toolkit\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "sno = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_LOC = 'C:\\\\Users\\\\pvaish10\\\\Desktop\\\\'  # !! Modify this to the CSV data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(filepath_or_buffer = DATA_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VerbCombos = ['VB',\n",
    "              'VBD',\n",
    "              'VBG',\n",
    "              'VBN',\n",
    "              'VBP',\n",
    "              'VBZ',\n",
    "              'WDT',\n",
    "              'WP',\n",
    "              'WP$',\n",
    "              'WRB',\n",
    "              'MD']\n",
    "\n",
    "questionTriples = ['CD-VB-VBN',\n",
    "                   'MD-PRP-VB' ,\n",
    "                   'MD-VB-CD' ,\n",
    "                   'NN-IN-DT' ,\n",
    "                   'PRP-VB-PRP' ,\n",
    "                   'PRP-WP-NNP' ,\n",
    "                   'VB-CD-VB' ,\n",
    "                   'VB-PRP-WP' ,\n",
    "                   'VBZ-DT-NN' ,\n",
    "                   'WP-VBZ-DT' ,\n",
    "                   'WP-VBZ-NNP' ,\n",
    "                   'WRB-MD-VB']\n",
    "\n",
    "statementTriples = ['DT-JJ-NN',\n",
    "                   'DT-NN-VBZ',\n",
    "                   'DT-NNP-NNP',\n",
    "                   'IN-DT-NN',\n",
    "                   'IN-NN-NNS',\n",
    "                   'MD-VB-VBN',\n",
    "                   'NNP-IN-NNP',\n",
    "                   'NNP-NNP-NNP',\n",
    "                   'NNP-VBZ-DT',\n",
    "                   'NNP-VBZ-NNP',\n",
    "                   'NNS-IN-DT',\n",
    "                   'VB-VBN-IN',\n",
    "                   'VBZ-DT-JJ']\n",
    "\n",
    "\n",
    "startTuples = ['NNS-DT',\n",
    "               'WP-VBZ',\n",
    "               'WRB-MD']                      \n",
    "\n",
    "endTuples = ['IN-NN',\n",
    "             'VB-VBN',\n",
    "             'VBZ-NNP'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "# word stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_sentence(sentence):\n",
    "    sentence = sentence.strip(\",\")\n",
    "    sentence = ''.join(filter(lambda x: x in string.printable, sentence))  #strip out non-alpha-numerix\n",
    "    sentence = sentence.translate(str.maketrans('','',string.punctuation)) #strip punctuation\n",
    "    return(sentence)\n",
    "\n",
    "# Pass in a list of strings (i.e. PoS types) and the sentence to check PoS types for\n",
    "# check if *Any Pair Combo* of the PoS types list exists in the sentence PoS types\n",
    "# return a count of occurrence\n",
    "def exists_pair_combos(comboCheckList, sentence):\n",
    "    pos = get_pos(sentence)\n",
    "    tag_string = \"-\".join([ i[1] for i in pos ])\n",
    "    combo_list = []\n",
    "    \n",
    "    for pair in itertools.permutations(comboCheckList,2):\n",
    "        if(pair[0] == \"MD\"):  # * Kludge - strip off leading MD *\n",
    "            pair = [\"\",\"\"]\n",
    "        combo_list.append(\"-\".join(pair))\n",
    "    \n",
    "    if any(code in tag_string for code in combo_list):\n",
    "\t    return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Parts Of Speech\n",
    "def get_pos(sentence):\n",
    "    sentenceParsed = nltk.word_tokenize(sentence)\n",
    "    return(nltk.pos_tag(sentenceParsed))\n",
    "    \n",
    "# Count Q-Marks    \n",
    "def count_qmark(sentence):\n",
    "    return(sentence.count(\"?\") )\n",
    "    \n",
    "# Count a specific POS-Type\n",
    "#VBG = count_POSType(pos,'VBG')\n",
    "def count_POSType(pos, ptype):\n",
    "    count = 0\n",
    "    tags = [ i[1] for i in pos ]\n",
    "    return(tags.count(ptype))\n",
    "    #if ptype in tags:\n",
    "    #    VBG = 1\n",
    "    #return(VBG)\n",
    "    \n",
    "# Does Verb occur before first Noun\n",
    "def exists_vb_before_nn(pos):\n",
    "    pos_tags = [ i[1] for i in pos ]\n",
    "    #Strip the Verbs to all just \"V\"\n",
    "    pos_tags = [ re.sub(r'V.*','V', str) for str in pos_tags ]\n",
    "    #Strip the Nouns to all just \"NN\"\n",
    "    pos_tags = [ re.sub(r'NN.*','NN', str) for str in pos_tags ]\n",
    "    \n",
    "    vi =99\n",
    "    ni =99\n",
    "    mi =99\n",
    "    \n",
    "    #Get first NN index\n",
    "    if \"NN\" in pos_tags:\n",
    "        ni = pos_tags.index(\"NN\")\n",
    "    #Get first V index\n",
    "    if \"V\" in pos_tags:\n",
    "        vi = pos_tags.index(\"V\")\n",
    "    #get Modal Index\n",
    "    if \"MD\" in pos_tags:\n",
    "        mi = pos_tags.index(\"MD\")\n",
    "\n",
    "    if vi < ni or mi < ni :\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "# Stemmed sentence ends in \"NN-NN\"?  \n",
    "def exists_stemmed_end_NN(stemmed):\n",
    "    stemmedEndNN = 0\n",
    "    stemmed_end = get_first_last_tuples(\" \".join(stemmed))[1]\n",
    "    if stemmed_end == \"NN-NN\":\n",
    "        stemmedEndNN = 1\n",
    "    return(stemmedEndNN)\n",
    "\n",
    "# Go through the predefined list of start-tuples, 1 / 0 if given startTuple occurs in the list\n",
    "def exists_startTuple(startTuple):    \n",
    "    exists_startTuples = []\n",
    "    for tstring in startTuples:  #startTuples defined as global var\n",
    "        if startTuple in tstring:\n",
    "            exists_startTuples.append(1)\n",
    "        else:\n",
    "            exists_startTuples.append(0)\n",
    "        return(exists_startTuples)\n",
    "\n",
    "# Go through the predefined list of end-tuples, 1 / 0 if given Tuple occurs in the list    \n",
    "def exists_endTuple(endTuple):\n",
    "    exists_endTuples = []\n",
    "    for tstring in endTuples:    #endTuples defined as global var\n",
    "        if endTuple in tstring:\n",
    "            exists_endTuples.append(1)\n",
    "        else:\n",
    "            exists_endTuples.append(0)\n",
    "    return(exists_endTuples)\n",
    "\n",
    "#loop round list of triples and construct a list of binary 1/0 vals if triples occur in list\n",
    "def exists_triples(triples, tripleSet):\n",
    "    exists = []\n",
    "    for tstring in tripleSet:   \n",
    "        if tstring in triples:\n",
    "            exists.append(1)\n",
    "        else:\n",
    "            exists.append(0)\n",
    "    return(exists)\n",
    "\n",
    "# Get a sentence and spit out the POS triples\n",
    "def get_triples(pos):\n",
    "    list_of_triple_strings = []\n",
    "    pos = [ i[1] for i in pos ]  # extract the 2nd element of the POS tuples in list\n",
    "    n = len(pos)\n",
    "    \n",
    "    if n > 2:  # need to have three items\n",
    "        for i in range(0,n-2):\n",
    "            t = \"-\".join(pos[i:i+3]) # pull out 3 list item from counter, convert to string\n",
    "            list_of_triple_strings.append(t)\n",
    "    return list_of_triple_strings \n",
    "        \n",
    "def get_first_last_tuples(sentence):\n",
    "    first_last_tuples = []\n",
    "    sentenceParsed = nltk.word_tokenize(sentence)\n",
    "    pos = nltk.pos_tag(sentenceParsed) #Parts Of Speech\n",
    "    pos = [ i[1] for i in pos ]  # extract the 2nd element of the POS tuples in list\n",
    "    \n",
    "    n = len(pos)\n",
    "    first = \"\"\n",
    "    last = \"\"\n",
    "    \n",
    "    if n > 1:  # need to have three items\n",
    "        first = \"-\".join(pos[0:2]) # pull out first 2 list items\n",
    "        last = \"-\".join(pos[-2:]) # pull out last 2 list items\n",
    "    \n",
    "    first_last_tuples = [first, last]\n",
    "    return first_last_tuples\n",
    "\n",
    "def lemmatize(sentence):    \n",
    "    \"\"\"\n",
    "    pass  in  a sentence as a string, return just core text that has been \"lematised\"\n",
    "    stop words are removed - could effect ability to detect if this is a question or answer\n",
    "    - depends on import lemma = nltk.wordnet.WordNetLemmatizer() and from nltk.corpus import stopwords\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w.lower())  # also set lowercase\n",
    "    lem = []        \n",
    "    for w in filtered_sentence:\n",
    "        lem.append(lemma.lemmatize(w))\n",
    "  \n",
    "    return lem    \n",
    "\n",
    "def stematize(sentence):\n",
    "    \"\"\"\n",
    "    pass  in  a sentence as a string, return just core text stemmed\n",
    "    stop words are removed - could effect ability to detect if this is a question or answer\n",
    "    - depends on import sno = nltk.stem.SnowballStemmer('english') and from nltk.corpus import stopwords\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    stemmed = []        \n",
    "    for w in filtered_sentence:\n",
    "        stemmed.append(sno.stem(w))\n",
    "  \n",
    "    return stemmed    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Count number of question marks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['qMark'] = train.apply(lambda row: count_qmark(row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['wordCount'] = train.apply(lambda row: len(row['SENTENCE'].split()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "train['text_without_stopwords'] = (train['SENTENCE'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop) ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['word_tokens'] = train.apply(lambda row: nltk.word_tokenize(row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['stemmed']  =  train.apply(lambda row: stematize(row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['lemma'] = train.apply(lambda row: lemmatize(row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['stemmedCount'] = train.apply(lambda row: len(row['stemmed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['lemmaCount'] = train.apply(lambda row: len(row['lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemma or Stemm???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['qVerbCombo'] = train.apply(lambda row: exists_pair_combos(VerbCombos,row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train[\"stemmedEndNN\"] = train.apply(lambda row: exists_stemmed_end_NN(row['stemmed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"lemmaEndNN\"] = train.apply(lambda row: exists_stemmed_end_NN(row['lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['pos'] = train.apply(lambda row: get_pos(row['SENTENCE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ptype in [\"VBG\", \"VBZ\", \"NNP\", \"NN\", \"NNS\", \"NNPS\",\"PRP\", \"CD\" ]:\n",
    "        train['_' + str(ptype)] = train['pos'].apply(lambda row: count_POSType(row,ptype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['verbBeforeNoun'] = train.apply(lambda row: exists_vb_before_nn(row['pos']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['get_triples'] = train.apply(lambda row: get_triples(row['pos']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Rearrange the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[['SENTENCE', 'qMark', 'wordCount', 'text_without_stopwords',\n",
    "       'lemma', 'lemmaCount', 'qVerbCombo', 'lemmaEndNN', 'pos', '_VBG',\n",
    "       '_VBZ', '_NNP', '_NN', '_NNS', '_NNPS', '_PRP', '_CD', 'verbBeforeNoun',\n",
    "       'get_triples', 'CLASS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['text_without_stopwords'] = class_le.fit_transform(train['text_without_stopwords'].values)\n",
    "train['SENTENCE'] = class_le.fit_transform(train['SENTENCE'].values)\n",
    "train['lemma'] = class_le.fit_transform(train['lemma'].values)\n",
    "train['get_triples'] = class_le.fit_transform(train['get_triples'].values)\n",
    "train['CLASS'] = class_le.fit_transform(train['CLASS'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(train)) < 0.8\n",
    "train_1 = train[msk]\n",
    "test = train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(train_1))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the test dataset without the CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_1 = test[test.columns.difference(['CLASS'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Join the two datasets and add a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine = pd.concat([train_1.drop('CLASS',1),test_1])\n",
    "response = train_1['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(combine.shape)\n",
    "print(response.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_subset = combine[['SENTENCE','qMark', 'wordCount', 'text_without_stopwords', 'lemma', 'lemmaCount', 'qVerbCombo',\n",
    "       'lemmaEndNN', '_VBG', '_VBZ', '_NNP', '_NN', '_NNS', '_NNPS',\n",
    "       '_PRP', '_CD', 'verbBeforeNoun', 'get_triples']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_subset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the target variable and the training and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_subset[:train_1.shape[0]]\n",
    "test_2 = train_subset[train_1.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (str(len(train_1))+\" rows for training set\")\n",
    "print (str(len(test_2))+\" rows for test set\")\n",
    "print (str(len(response))+\" rows for target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkResponse = test['CLASS']\n",
    "print(len(checkResponse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, response)\n",
    "test_y_1 = logreg.predict(test_2)\n",
    "#Comparing the test column with labels against the predictions\n",
    "accuracy_score(checkResponse, test_y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix for Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_names = ['No', 'Yes']\n",
    "print(classification_report(checkResponse, test_y_1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform back to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_Transform = class_le.inverse_transform(test_y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
