{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, BatchNormalization\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "import cv2\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 75\n",
    "input_channels = 3\n",
    "img_size = (75, 75)\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "training = True\n",
    "\n",
    "ensemble_voting = False  # If True, use voting for model ensemble, otherwise use averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_json(\"data/iceberg/train.json\")\n",
    "test = pd.read_json(\"data/iceberg/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_1']])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_2']])\n",
    "\n",
    "X_train = np.concatenate([x_band1[:, :, :, np.newaxis],\n",
    "                          x_band2[:, :, :, np.newaxis],\n",
    "                          ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "target_train=train['is_iceberg']\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test['band_1']])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test['band_2']])\n",
    "\n",
    "X_test = np.concatenate([x_band1[:, :, :, np.newaxis],\n",
    "                         x_band2[:, :, :, np.newaxis],\n",
    "                         ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "id_test = test['id'].values\n",
    "\n",
    "del test; del x_band1; del x_band2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=1)\n",
    "\n",
    "fold_count = 0\n",
    "\n",
    "y_full_test = []\n",
    "thres_sum = np.zeros(17, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  30\n",
      "Validating on 321 samples\n",
      "Training on 1283 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_21 to have 2 dimensions, but got array with shape (128, 75, 75, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-bde2c171e35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                             validation_steps=(len(df_valid) // batch_size) + 1)\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1625\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1307\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1310\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1311\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_21 to have 2 dimensions, but got array with shape (128, 75, 75, 3)"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X_train):\n",
    "    \n",
    "    fold_count += 1\n",
    "    print('Fold ', fold_count)\n",
    "    \n",
    "    df_train = X_train[train_index]; df_valid = X_train[test_index]\n",
    "    y_train = X_test[train_index]; y_valid = X_train[test_index]\n",
    "\n",
    "\n",
    "    def transformations(src, choice):\n",
    "        if choice == 0:\n",
    "            # Rotate 90\n",
    "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n",
    "        if choice == 1:\n",
    "            # Rotate 90 and flip horizontally\n",
    "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n",
    "            src = cv2.flip(src, flipCode=1)\n",
    "        if choice == 2:\n",
    "            # Rotate 180\n",
    "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_180)\n",
    "        if choice == 3:\n",
    "            # Rotate 180 and flip horizontally\n",
    "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_180)\n",
    "            src = cv2.flip(src, flipCode=1)\n",
    "        if choice == 4:\n",
    "            # Rotate 90 counter-clockwise\n",
    "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        if choice == 5:\n",
    "            # Rotate 90 counter-clockwise and flip horizontally\n",
    "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            src = cv2.flip(src, flipCode=1)\n",
    "        return src\n",
    "\n",
    "    df_valid = df_train_data.ix[test_index]\n",
    "    print('Validating on {} samples'.format(len(df_valid)))\n",
    "\n",
    "\n",
    "    def valid_generator():\n",
    "        while True:\n",
    "            for start in range(0, len(df_valid), batch_size):\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "                end = min(start + batch_size, len(df_valid))\n",
    "                df_valid_batch = df_valid[start:end]\n",
    "                for f, tags in df_valid_batch.values:\n",
    "                    #img = cv2.imread('input/train-jpg/{}.jpg'.format(f))\n",
    "                    img = cv2.resize(img, (input_size, input_size))\n",
    "                    img = transformations(img, np.random.randint(6))\n",
    "                    targets = np.zeros(17)\n",
    "                    for t in tags.split(' '):\n",
    "                        targets[label_map[t]] = 1\n",
    "                    x_batch.append(img)\n",
    "                    y_batch.append(targets)\n",
    "                x_batch = np.array(x_batch, np.float32)\n",
    "                y_batch = np.array(y_batch, np.uint8)\n",
    "                yield x_batch, y_batch\n",
    "\n",
    "\n",
    "    df_train = df_train_data.ix[train_index]\n",
    "    if training:\n",
    "        print('Training on {} samples'.format(len(df_train)))\n",
    "\n",
    "\n",
    "    def train_generator():\n",
    "        while True:\n",
    "            for start in range(0, len(df_train), batch_size):\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "                end = min(start + batch_size, len(df_train))\n",
    "                df_train_batch = df_train[start:end]\n",
    "                for f, tags in X_train.values:\n",
    "                    #img = cv2.imread('input/train-jpg/{}.jpg'.format(f))\n",
    "                    img = cv2.resize(img, (input_size, input_size))\n",
    "                    img = transformations(img, np.random.randint(6))\n",
    "                    targets = np.zeros(17)\n",
    "                    for t in tags.split(' '):\n",
    "                        targets[label_map[t]] = 1\n",
    "                    x_batch.append(img)\n",
    "                    y_batch.append(targets)\n",
    "                x_batch = np.array(x_batch, np.float32)\n",
    "                y_batch = np.array(y_batch, np.uint8)\n",
    "                yield x_batch, y_batch\n",
    "\n",
    "    base_model = VGG19(include_top=False,\n",
    "                       weights='imagenet', \n",
    "                       input_shape=(input_size, input_size, input_channels))\n",
    "\n",
    "    model = Sequential()\n",
    "    # Batchnorm input\n",
    "    model.add(BatchNormalization(input_shape=(input_size, input_size, input_channels)))\n",
    "    # Base model\n",
    "    model.add(base_model)\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    opt = Adam(lr=1e-4)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               min_delta=1e-4),\n",
    "                 ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.1,\n",
    "                                   patience=2,\n",
    "                                   cooldown=2,\n",
    "                                   verbose=1),\n",
    "                 ModelCheckpoint(filepath='weights/best_weights.fold_' + str(fold_count) + '.hdf5',\n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True)]\n",
    "\n",
    "    if training:\n",
    "        model.fit_generator(generator=train_generator(),\n",
    "                            steps_per_epoch=(len(df_train) // batch_size) + 1,\n",
    "                            epochs=epochs,\n",
    "                            verbose=2,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_data=valid_generator(),\n",
    "                            validation_steps=(len(df_valid) // batch_size) + 1)\n",
    "\n",
    "\n",
    "    def optimise_f2_thresholds(y, p, verbose=True, resolution=100):\n",
    "        def mf(x):\n",
    "            p2 = np.zeros_like(p)\n",
    "            for i in range(17):\n",
    "                p2[:, i] = (p[:, i] > x[i]).astype(np.int)\n",
    "            score = fbeta_score(y, p2, beta=2, average='samples')\n",
    "            return score\n",
    "\n",
    "        x = [0.2] * 17\n",
    "        for i in range(17):\n",
    "            best_i2 = 0\n",
    "            best_score = 0\n",
    "            for i2 in range(resolution):\n",
    "                i2 /= float(resolution)\n",
    "                x[i] = i2\n",
    "                score = mf(x)\n",
    "                if score > best_score:\n",
    "                    best_i2 = i2\n",
    "                    best_score = score\n",
    "            x[i] = best_i2\n",
    "            if verbose:\n",
    "                print(i, best_i2, best_score)\n",
    "        return x\n",
    "\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_weights(filepath='weights/best_weights.fold_' + str(fold_count) + '.hdf5')\n",
    "\n",
    "    p_valid = model.predict_generator(generator=valid_generator(),\n",
    "                                      steps=(len(df_valid) // batch_size) + 1)\n",
    "\n",
    "    y_valid = []\n",
    "    for f, tags in df_valid.values:\n",
    "        targets = np.zeros(17)\n",
    "        for t in tags.split(' '):\n",
    "            targets[label_map[t]] = 1\n",
    "        y_valid.append(targets)\n",
    "    y_valid = np.array(y_valid, np.uint8)\n",
    "\n",
    "    # Find optimal f2 thresholds for local validation set\n",
    "    thres = optimise_f2_thresholds(y_valid, p_valid, verbose=False)\n",
    "\n",
    "    print('F2 = {}'.format(fbeta_score(y_valid, np.array(p_valid) > thres, beta=2, average='samples')))\n",
    "\n",
    "    thres_sum += np.array(thres, np.float32)\n",
    "\n",
    "\n",
    "    def test_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(test), n_fold):\n",
    "                    x_batch = []\n",
    "                    end = min(start + n_fold, len(test))\n",
    "                    for img in test[start:end]:\n",
    "                        new_img = cv2.resize(img, img_size)\n",
    "                        x_batch.append(new_img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    yield x_batch\n",
    "\n",
    "    # 6-fold TTA\n",
    "    p_full_test = []\n",
    "    for i in range(6):\n",
    "        p_test = model.predict_generator(generator=test_generator(transformation=i),\n",
    "                                         steps=(len(X_test) // batch_size) + 1)\n",
    "        p_full_test.append(p_test)\n",
    "\n",
    "    p_test = np.array(p_full_test[0])\n",
    "    for i in range(1, 6):\n",
    "        p_test += np.array(p_full_test[i])\n",
    "    p_test /= 6\n",
    "\n",
    "    y_full_test.append(p_test)\n",
    "\n",
    "result = np.array(y_full_test[0])\n",
    "if ensemble_voting:\n",
    "    for f in range(len(y_full_test[0])):  # For each file\n",
    "        for tag in range(17):  # For each tag\n",
    "            preds = []\n",
    "            for fold in range(n_folds):  # For each fold\n",
    "                preds.append(y_full_test[fold][f][tag])\n",
    "            pred = Counter(preds).most_common(1)[0][0]  # Most common tag prediction among folds\n",
    "            result[f][tag] = pred\n",
    "else:\n",
    "    for fold in range(1, n_folds):\n",
    "        result += np.array(y_full_test[fold])\n",
    "    result /= n_folds\n",
    "result = pd.DataFrame(result, columns=labels)\n",
    "\n",
    "preds = []\n",
    "thres = (thres_sum / n_folds).tolist()\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "    a = result.ix[[i]]\n",
    "    a = a.apply(lambda x: x > thres, axis=1)\n",
    "    a = a.transpose()\n",
    "    a = a.loc[a[i] == True]\n",
    "    ' '.join(list(a.index))\n",
    "    preds.append(' '.join(list(a.index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
