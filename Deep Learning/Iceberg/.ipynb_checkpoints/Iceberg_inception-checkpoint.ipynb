{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_json(\"data/iceberg/train.json\")\n",
    "test = pd.read_json(\"data/iceberg/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_1']])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_2']])\n",
    "\n",
    "X_train = np.concatenate([x_band1[:, :, :, np.newaxis],\n",
    "                          x_band2[:, :, :, np.newaxis],\n",
    "                          ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "target_train=train['is_iceberg']\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test['band_1']])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test['band_2']])\n",
    "\n",
    "X_test = np.concatenate([x_band1[:, :, :, np.newaxis],\n",
    "                         x_band2[:, :, :, np.newaxis],\n",
    "                         ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "id_test = test['id'].values\n",
    "\n",
    "del test; del x_band1; del x_band2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 5, 5, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 21,804,845\n",
      "Trainable params: 21,770,407\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define CNN Model Architecture (Kaggle can't access the weights file)\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "img_channels = 3\n",
    "img_dim = (img_height, img_width, img_channels)\n",
    "\n",
    "def inceptionv3(img_dim=img_dim):\n",
    "    input_tensor = Input(shape=img_dim)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=img_dim)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = inceptionv3()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Model and predict\n",
    "def train_model(model, batch_size, epochs, img_size, x, y, test, n_fold, kf):\n",
    "        \n",
    "    train_scores = []; valid_scores = []\n",
    "    preds_test = np.zeros(len(test), dtype = np.float)\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train = x[train_index]; x_valid = x[test_index]\n",
    "        y_train = y[train_index]; y_valid = y[test_index]\n",
    "\n",
    "        def augment(src, choice):\n",
    "            if choice == 0:\n",
    "                # Rotate 90\n",
    "                src = np.rot90(src, 1)\n",
    "            if choice == 1:\n",
    "                # flip vertically\n",
    "                src = np.flipud(src)\n",
    "            if choice == 2:\n",
    "                # Rotate 180\n",
    "                src = np.rot90(src, 2)\n",
    "            if choice == 3:\n",
    "                # flip horizontally\n",
    "                src = np.fliplr(src)\n",
    "            if choice == 4:\n",
    "                # Rotate 90 counter-clockwise\n",
    "                src = np.rot90(src, 3)\n",
    "            if choice == 5:\n",
    "                # Rotate 180 and flip horizontally\n",
    "                src = np.rot90(src, 2)\n",
    "                src = np.fliplr(src)\n",
    "            return src\n",
    "\n",
    "        def train_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_train), batch_size):\n",
    "                    x_batch = []\n",
    "                    end = min(start + batch_size, len(x_train))\n",
    "                    y_batch = y_train[start:end]\n",
    "                    for img in x_train[start:end]:\n",
    "                        new_img = cv2.resize(img, img_size)\n",
    "                        new_img = augment(new_img, np.random.randint(6))\n",
    "                        x_batch.append(new_img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def valid_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_valid), batch_size):\n",
    "                    x_batch = []\n",
    "                    end = min(start + batch_size, len(x_valid))\n",
    "                    y_batch = y_valid[start:end]\n",
    "                    for img in x_valid[start:end]:\n",
    "                        new_img = cv2.resize(img, img_size)\n",
    "                        x_batch.append(new_img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def test_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(test), n_fold):\n",
    "                    x_batch = []\n",
    "                    end = min(start + n_fold, len(test))\n",
    "                    for img in test[start:end]:\n",
    "                        new_img = cv2.resize(img, img_size)\n",
    "                        x_batch.append(new_img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    yield x_batch\n",
    "                    \n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, cooldown=1, \n",
    "                               verbose=1, min_lr=1e-7),\n",
    "             ModelCheckpoint(filepath='inception.fold_' + str(i) + '.hdf5', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=True, mode='auto')]\n",
    "\n",
    "        train_steps = len(x_train) / batch_size\n",
    "        valid_steps = len(x_valid) / batch_size\n",
    "        test_steps = len(test) / n_fold\n",
    "        \n",
    "        model = model\n",
    "\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        model.fit_generator(train_generator(), train_steps, epochs=epochs, verbose=1, \n",
    "                            callbacks=callbacks, validation_data=valid_generator(), \n",
    "                            validation_steps=valid_steps)\n",
    "\n",
    "        model.load_weights(filepath='inception.fold_' + str(i) + '.hdf5')\n",
    "\n",
    "        \n",
    "        print('----------------------------------------')\n",
    "        print('Running train evaluation on fold {}'.format(i))\n",
    "        train_score = model.evaluate_generator(train_generator(), steps=train_steps)        \n",
    "        print('Running validation evaluation on fold {}'.format(i))\n",
    "        valid_score = model.evaluate_generator(valid_generator(), steps=valid_steps)\n",
    "        print('----------------------------------------')   \n",
    "        \n",
    "        print('Train loss: {:0.5f}\\n Train acc: {:0.5f} for fold {}'.format(train_score[0],\n",
    "                                                                            train_score[1], i))\n",
    "        print('Valid loss: {:0.5f}\\n Valid acc: {:0.5f} for fold {}'.format(valid_score[0],\n",
    "                                                                            valid_score[1], i))\n",
    "        print('----------------------------------------')\n",
    "\n",
    "        train_scores.append(train_score[1])\n",
    "        valid_scores.append(valid_score[1])\n",
    "        print('Avg Train Acc: {:0.5f}\\nAvg Valid Acc: {:0.5f} after {} folds'.format\n",
    "              (np.mean(train_scores), np.mean(valid_scores), i))\n",
    "        print('----------------------------------------')\n",
    "        \n",
    "        print('Running test predictions with fold {}'.format(i))        \n",
    "        preds_test_fold = model.predict_generator(generator=test_generator(),\n",
    "                                              steps=test_steps, verbose=1)[:, -1]\n",
    "\n",
    "        preds_test += preds_test_fold\n",
    "\n",
    "        print('\\n\\n')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i <= n_fold:\n",
    "            print('Now beginning training for fold {}\\n\\n'.format(i))\n",
    "        else:\n",
    "            print('Finished training!')\n",
    "\n",
    "    preds_test /= n_fold\n",
    "\n",
    "    return preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.7069Epoch 00000: val_loss improved from inf to 7.66193, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 78s - loss: 0.5600 - acc: 0.7086 - val_loss: 7.6619 - val_acc: 0.5196\n",
      "Epoch 2/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.7987Epoch 00001: val_loss improved from 7.66193 to 6.15245, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 69s - loss: 0.4252 - acc: 0.7998 - val_loss: 6.1524 - val_acc: 0.5196\n",
      "Epoch 3/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8258Epoch 00002: val_loss improved from 6.15245 to 0.60710, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 69s - loss: 0.3716 - acc: 0.8268 - val_loss: 0.6071 - val_acc: 0.7477\n",
      "Epoch 4/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8464Epoch 00003: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.3222 - acc: 0.8473 - val_loss: 0.7145 - val_acc: 0.6374\n",
      "Epoch 5/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.8699Epoch 00004: val_loss improved from 0.60710 to 0.54846, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 68s - loss: 0.2963 - acc: 0.8706 - val_loss: 0.5485 - val_acc: 0.7458\n",
      "Epoch 6/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9110Epoch 00005: val_loss improved from 0.54846 to 0.41648, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 68s - loss: 0.2260 - acc: 0.9115 - val_loss: 0.4165 - val_acc: 0.8187\n",
      "Epoch 7/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9195Epoch 00006: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.2279 - acc: 0.9199 - val_loss: 0.4327 - val_acc: 0.8168\n",
      "Epoch 8/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9176\n",
      "Epoch 00007: reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 00007: val_loss did not improve\n",
      "179/178 [==============================] - 64s - loss: 0.2092 - acc: 0.9181 - val_loss: 0.7002 - val_acc: 0.7832\n",
      "Epoch 9/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9335Epoch 00008: val_loss improved from 0.41648 to 0.31884, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 67s - loss: 0.1831 - acc: 0.9339 - val_loss: 0.3188 - val_acc: 0.8991\n",
      "Epoch 10/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9569Epoch 00009: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.1274 - acc: 0.9572 - val_loss: 0.3211 - val_acc: 0.9009\n",
      "Epoch 11/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9644Epoch 00010: val_loss improved from 0.31884 to 0.31449, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 68s - loss: 0.1110 - acc: 0.9646 - val_loss: 0.3145 - val_acc: 0.9065\n",
      "Epoch 12/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9728Epoch 00011: val_loss improved from 0.31449 to 0.30952, saving model to inception.fold_1.hdf5\n",
      "179/178 [==============================] - 66s - loss: 0.0931 - acc: 0.9730 - val_loss: 0.3095 - val_acc: 0.9028\n",
      "Epoch 13/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9747Epoch 00012: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0769 - acc: 0.9749 - val_loss: 0.3190 - val_acc: 0.9009\n",
      "Epoch 14/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9785\n",
      "Epoch 00013: reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 00013: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0641 - acc: 0.9786 - val_loss: 0.3267 - val_acc: 0.9028\n",
      "Epoch 15/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9888Epoch 00014: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0558 - acc: 0.9888 - val_loss: 0.3226 - val_acc: 0.8953\n",
      "Epoch 16/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9906\n",
      "Epoch 00015: reducing learning rate to 1e-07.\n",
      "Epoch 00015: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0512 - acc: 0.9907 - val_loss: 0.3225 - val_acc: 0.9009\n",
      "Epoch 00015: early stopping\n",
      "----------------------------------------\n",
      "Running train evaluation on fold 1\n",
      "Running validation evaluation on fold 1\n",
      "----------------------------------------\n",
      "Train loss: 0.12342\n",
      " Train acc: 0.95510 for fold 1\n",
      "Valid loss: 0.30952\n",
      " Valid acc: 0.90280 for fold 1\n",
      "----------------------------------------\n",
      "Avg Train Acc: 0.95510\n",
      "Avg Valid Acc: 0.90280 after 1 folds\n",
      "----------------------------------------\n",
      "Running test predictions with fold 1\n",
      "2808/2808 [==============================] - 219s   \n",
      "\n",
      "\n",
      "\n",
      "Now beginning training for fold 2\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8418Epoch 00000: val_loss improved from inf to 0.46954, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 70s - loss: 0.3795 - acc: 0.8426 - val_loss: 0.4695 - val_acc: 0.7832\n",
      "Epoch 2/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.8839Epoch 00001: val_loss improved from 0.46954 to 0.34757, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 67s - loss: 0.2834 - acc: 0.8845 - val_loss: 0.3476 - val_acc: 0.8467\n",
      "Epoch 3/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9148Epoch 00002: val_loss improved from 0.34757 to 0.31819, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 67s - loss: 0.2450 - acc: 0.9153 - val_loss: 0.3182 - val_acc: 0.8467\n",
      "Epoch 4/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9148Epoch 00003: val_loss did not improve\n",
      "179/178 [==============================] - 62s - loss: 0.2384 - acc: 0.9153 - val_loss: 0.6504 - val_acc: 0.7963\n",
      "Epoch 5/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9438\n",
      "Epoch 00004: reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 00004: val_loss did not improve\n",
      "179/178 [==============================] - 65s - loss: 0.1786 - acc: 0.9441 - val_loss: 0.4714 - val_acc: 0.8355\n",
      "Epoch 6/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9410Epoch 00005: val_loss improved from 0.31819 to 0.26242, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 67s - loss: 0.1924 - acc: 0.9413 - val_loss: 0.2624 - val_acc: 0.8879\n",
      "Epoch 7/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9710Epoch 00006: val_loss improved from 0.26242 to 0.25510, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 67s - loss: 0.1095 - acc: 0.9711 - val_loss: 0.2551 - val_acc: 0.8879\n",
      "Epoch 8/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9803Epoch 00007: val_loss improved from 0.25510 to 0.24483, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 67s - loss: 0.0946 - acc: 0.9804 - val_loss: 0.2448 - val_acc: 0.8897\n",
      "Epoch 9/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9785Epoch 00008: val_loss did not improve\n",
      "179/178 [==============================] - 62s - loss: 0.0886 - acc: 0.9786 - val_loss: 0.2485 - val_acc: 0.8879\n",
      "Epoch 10/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9803Epoch 00009: val_loss improved from 0.24483 to 0.23569, saving model to inception.fold_2.hdf5\n",
      "179/178 [==============================] - 68s - loss: 0.0829 - acc: 0.9804 - val_loss: 0.2357 - val_acc: 0.8935\n",
      "Epoch 11/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9850Epoch 00010: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0756 - acc: 0.9851 - val_loss: 0.2431 - val_acc: 0.8916\n",
      "Epoch 12/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9878\n",
      "Epoch 00011: reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 00011: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0588 - acc: 0.9879 - val_loss: 0.2628 - val_acc: 0.8953\n",
      "Epoch 13/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9916Epoch 00012: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0581 - acc: 0.9916 - val_loss: 0.2738 - val_acc: 0.8953\n",
      "Epoch 14/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9869\n",
      "Epoch 00013: reducing learning rate to 1e-07.\n",
      "Epoch 00013: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0519 - acc: 0.9870 - val_loss: 0.2711 - val_acc: 0.8916\n",
      "Epoch 00013: early stopping\n",
      "----------------------------------------\n",
      "Running train evaluation on fold 2\n",
      "Running validation evaluation on fold 2\n",
      "----------------------------------------\n",
      "Train loss: 0.10747\n",
      " Train acc: 0.95697 for fold 2\n",
      "Valid loss: 0.23569\n",
      " Valid acc: 0.89346 for fold 2\n",
      "----------------------------------------\n",
      "Avg Train Acc: 0.95603\n",
      "Avg Valid Acc: 0.89813 after 2 folds\n",
      "----------------------------------------\n",
      "Running test predictions with fold 2\n",
      "2808/2808 [==============================] - 226s   \n",
      "\n",
      "\n",
      "\n",
      "Now beginning training for fold 3\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9054Epoch 00000: val_loss improved from inf to 0.26886, saving model to inception.fold_3.hdf5\n",
      "179/178 [==============================] - 73s - loss: 0.2572 - acc: 0.9060 - val_loss: 0.2689 - val_acc: 0.8876\n",
      "Epoch 2/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9176Epoch 00001: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.2418 - acc: 0.9181 - val_loss: 0.4829 - val_acc: 0.8333\n",
      "Epoch 3/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9354\n",
      "Epoch 00002: reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 00002: val_loss did not improve\n",
      "179/178 [==============================] - 66s - loss: 0.1760 - acc: 0.9358 - val_loss: 0.4144 - val_acc: 0.8446\n",
      "Epoch 4/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9663Epoch 00003: val_loss improved from 0.26886 to 0.23995, saving model to inception.fold_3.hdf5\n",
      "179/178 [==============================] - 67s - loss: 0.1303 - acc: 0.9665 - val_loss: 0.2399 - val_acc: 0.9101\n",
      "Epoch 5/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9775Epoch 00004: val_loss improved from 0.23995 to 0.23185, saving model to inception.fold_3.hdf5\n",
      "179/178 [==============================] - 67s - loss: 0.0775 - acc: 0.9777 - val_loss: 0.2319 - val_acc: 0.9120\n",
      "Epoch 6/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9728Epoch 00005: val_loss improved from 0.23185 to 0.21423, saving model to inception.fold_3.hdf5\n",
      "179/178 [==============================] - 68s - loss: 0.0819 - acc: 0.9730 - val_loss: 0.2142 - val_acc: 0.9213\n",
      "Epoch 7/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9869Epoch 00006: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0434 - acc: 0.9870 - val_loss: 0.2205 - val_acc: 0.9195\n",
      "Epoch 8/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9860Epoch 00007: val_loss improved from 0.21423 to 0.20892, saving model to inception.fold_3.hdf5\n",
      "179/178 [==============================] - 66s - loss: 0.0485 - acc: 0.9860 - val_loss: 0.2089 - val_acc: 0.9213\n",
      "Epoch 9/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9934Epoch 00008: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0307 - acc: 0.9935 - val_loss: 0.2182 - val_acc: 0.9232\n",
      "Epoch 10/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9925\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 00009: val_loss did not improve\n",
      "179/178 [==============================] - 63s - loss: 0.0299 - acc: 0.9926 - val_loss: 0.2293 - val_acc: 0.9232\n",
      "Epoch 11/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9925Epoch 00010: val_loss did not improve\n",
      "179/178 [==============================] - 61s - loss: 0.0303 - acc: 0.9926 - val_loss: 0.2244 - val_acc: 0.9195\n",
      "Epoch 12/50\n",
      "178/178 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9888\n",
      "Epoch 00011: reducing learning rate to 1e-07.\n",
      "Epoch 00011: val_loss did not improve\n",
      "179/178 [==============================] - 61s - loss: 0.0336 - acc: 0.9888 - val_loss: 0.2260 - val_acc: 0.9232\n",
      "Epoch 00011: early stopping\n",
      "----------------------------------------\n",
      "Running train evaluation on fold 3\n",
      "Running validation evaluation on fold 3\n",
      "----------------------------------------\n",
      "Train loss: 0.07879\n",
      " Train acc: 0.97944 for fold 3\n",
      "Valid loss: 0.20892\n",
      " Valid acc: 0.92135 for fold 3\n",
      "----------------------------------------\n",
      "Avg Train Acc: 0.96384\n",
      "Avg Valid Acc: 0.90587 after 3 folds\n",
      "----------------------------------------\n",
      "Running test predictions with fold 3\n",
      "2808/2808 [==============================] - 217s   \n",
      "\n",
      "\n",
      "\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "epochs = 50\n",
    "n_fold = 3\n",
    "img_size = (img_height, img_width)\n",
    "kf = KFold(n_splits=n_fold, shuffle=True)\n",
    "\n",
    "prediction = train_model(model, batch_size, epochs, img_size, X_train, \n",
    "                                target_train, X_test, n_fold, kf)\n",
    "\n",
    "submit = pd.DataFrame({'id': id_test, 'is_iceberg': prediction.reshape((prediction.shape[0]))})\n",
    "submit.to_csv('./submission_inception.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
