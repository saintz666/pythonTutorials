{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading CSV files\n",
    "#Sometimes the CSV file contains padding spaces in front of the values. To ignore them use the skipinitialspaces parameter:\n",
    "\n",
    "pd.read_csv('file.csv', sep=';', skipinitialspace=True)\n",
    "#If the padding white spaces occur on both sides of the cell values we need to use a regular expression separator.\n",
    "# In this case, we need to use the ‘python’ processing engine, instead of the underlying native one, in order to avoid warnings. This will degrade the performance a bit:\n",
    "\n",
    "pd.read_csv('file.csv', sep='\\s*;\\s*', skipinitialspace=True, engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sometimes we need to sample the data before loading it, as it is too big to fit in memory.\n",
    "#  http://nikgrozev.com/2015/06/16/fast-and-simple-sampling-in-pandas-when-loading-data-from-files/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List unique values in a DataFrame column\n",
    "df['Column Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert Series datatype to numeric (will error if column has non-numeric values)\n",
    "pd.to_numeric(df['Column Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert Series datatype to numeric, changing non-numeric values to NaN\n",
    "pd.to_numeric(df['Column Name'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab DataFrame rows where column has certain values\n",
    "valuelist = ['value1', 'value2', 'value3']\n",
    "df = df[df.column.isin(valuelist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab DataFrame rows where column doesn't have certain values\n",
    "valuelist = ['value1', 'value2', 'value3']\n",
    "df = df[~df.column.isin(value_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete column from DataFrame\n",
    "del df['column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add labels to columns:\n",
    "df.columns = ['a', 'b', 'c']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop all but a and c columns:\n",
    "df[['a', 'c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get a NumPy array of index values:\n",
    "df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make column a the index\n",
    "df.set_index('a', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check if column a is already sorted by comparing initial and value-sorted indexes:\n",
    ">>> df.a.index.tolist()\n",
    "[0, 1, 2, 3, 4]\n",
    ">>> df.a.sort_values().index.tolist()\n",
    "[2, 4, 1, 0, 3]\n",
    ">>> df.a.index.tolist() == df.a.sort_values().index.tolist()\n",
    "False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sort along the index:\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Deduplicate c data points at the same a index, with the highest c value taking precedence:\n",
    "df['c'].reset_index().groupby('a').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stackoverflow: Pandas\n",
    "https://stackoverflow.com/questions/tagged/pandas\n",
    "\n",
    "###### Pandas Code Snippets\n",
    "http://pandascodesnippets.blogspot.ie/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select from DataFrame using criteria from multiple columns\n",
    "# (use `|` instead of `&` to do an OR)\n",
    "newdf = df[(df['column_one']>2004) & (df['column_two']==9)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding new column\n",
    "df.insert(loc=3, column='newCol', value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename several DataFrame columns\n",
    "df = df.rename(columns = {\n",
    "    'col1 old name':'col1 new name',\n",
    "    'col2 old name':'col2 new name',\n",
    "    'col3 old name':'col3 new name',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply functions to multiple columns\n",
    "cols = ['A', 'B', 'C']\n",
    "df[cols] = df[cols].applymap(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lower-case all DataFrame column names\n",
    "df.columns = map(str.lower, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Even more fancy DataFrame column re-naming\n",
    "# lower-case all DataFrame column names (for example)\n",
    "df.rename(columns=lambda x: x.split('.')[-1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop through rows in a DataFrame\n",
    "# (if you must)\n",
    "for index, row in df.iterrows():\n",
    "    print index, row['some column']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Much faster way to loop through DataFrame rows\n",
    "# if you can work with tuples\n",
    "for row in df.itertuples():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next few examples show how to work with text data in Pandas.\n",
    "# Full list of .str functions: http://pandas.pydata.org/pandas-docs/stable/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slice values in a DataFrame column (aka Series)\n",
    "df.column.str[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lower-case everything in a DataFrame column\n",
    "df.column_name = df.column_name.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get length of data in a DataFrame column\n",
    "df.column_name.str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort dataframe by multiple columns\n",
    "df = df.sort(['col1','col2','col3'],ascending=[1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top n for each group of columns in a sorted dataframe\n",
    "# (make sure dataframe is sorted first)\n",
    "top5 = df.groupby(['groupingcol1', 'groupingcol2']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab DataFrame rows where specific column is null/notnull\n",
    "newdf = df[df['column'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select from DataFrame using multiple keys of a hierarchical index\n",
    "df.xs(('index level 1 value','index level 2 value'), level=('level 1','level 2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change all NaNs to None (useful before\n",
    "# loading to a db)\n",
    "df = df.where((pd.notnull(df)), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count NaN\n",
    "df.shape[0] - df.dropna().shape[0]\n",
    "# selecting NaN rows\n",
    "df[df['B'].isnull()]\n",
    "# selecting not null rows\n",
    "df[df['B'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#chaining conditions\n",
    "\n",
    "df[(df['blood'] == 'B') | (df['blood'] == 'AB')]\n",
    "df[(df['blood'] == 'B') & (df['blood'] == 'AB')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More pre-db insert cleanup...make a pass through the dataframe, stripping whitespace\n",
    "# from strings and changing any empty values to None\n",
    "# (not especially recommended but including here b/c I had to do this in real life one time)\n",
    "df = df.applymap(lambda x: str(x).strip() if len(str(x).strip()) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get quick count of rows in a DataFrame\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pivot data (with flexibility about what what\n",
    "# becomes a column and what stays a row).\n",
    "# Syntax works on Pandas >= .14\n",
    "pd.pivot_table(\n",
    "  df,values='cell_value',\n",
    "  index=['col1', 'col2', 'col3'], #these stay as columns; will fail silently if any of these cols have null values\n",
    "  columns=['col4']) #data values in this column become their own column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change data type of DataFrame column\n",
    "df.column_name = df.column_name.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get rid of non-numeric values throughout a DataFrame:\n",
    "for col in refunds.columns.values:\n",
    "  refunds[col] = refunds[col].replace('[^0-9]+.-', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set DataFrame column values based on other column values \n",
    "df.loc[(df['column1'] == some_value) & (df['column2'] == some_other_value), ['column_to_change']] = new_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up missing values in multiple DataFrame columns\n",
    "df = df.fillna({\n",
    "    'col1': 'missing',\n",
    "    'col2': '99.999',\n",
    "    'col3': '999',\n",
    "    'col4': 'missing',\n",
    "    'col5': 'missing',\n",
    "    'col6': '99'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#renaming columns\n",
    "df.rename(columns={\n",
    "    'S': 'Strength',\n",
    "    'W': 'Weak',\n",
    "    'O': 'Opportunity',\n",
    "    'T': 'Threat',\n",
    "})\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate two DataFrame columns into a new, single column\n",
    "# (useful when dealing with composite keys, for example)\n",
    "\n",
    "df['newcol'] = df['col1'].astype(str) + df['col2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing calculations with DataFrame columns that have missing values\n",
    "# In example below, swap in 0 for df['col1'] cells that contain null\n",
    "df['new_col'] = np.where(pd.isnull(df['col1']),0,df['col1']) + df['col2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split delimited values in a DataFrame column into two new columns\n",
    "df['new_col1'], df['new_col2'] = zip(*df['original_col'].apply(lambda x: x.split(': ', 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collapse hierarchical column indexes\n",
    "df.columns = df.columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert Django queryset to DataFrame\n",
    "qs = DjangoModelName.objects.all()\n",
    "q = qs.values()\n",
    "df = pd.DataFrame.from_records(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from a Python dictionary\n",
    "df = pd.DataFrame(list(a_dictionary.items()), columns = ['column1', 'column2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a report of all duplicate records in a dataframe, based on specific columns\n",
    "dupes = df[df.duplicated(['col1', 'col2', 'col3'], keep=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up formatting so larger numbers aren't displayed in scientific notation (h/t @thecapacity)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Excellent Resources\n",
    "\n",
    "https://www.maxmasnick.com/projects/scipy-tips/\n",
    "\n",
    "https://chrisalbon.com/\n",
    "\n",
    "http://www.swegler.com/becky/blog/2014/08/06/useful-pandas-snippets/\n",
    "\n",
    "https://jeffdelaney.me/blog/useful-snippets-in-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing a CSV File\n",
    "\n",
    "df = pd.read_csv('pizza.csv')\n",
    "#Need to parse dates? Just pass in the corresponding column name(s).\n",
    "\n",
    "df = pd.read_csv('pizza.csv', parse_dates=['dates'])\n",
    "#Only need a few specific columns?\n",
    "\n",
    "df = pd.read_csv('pizza.csv', usecols=['foo', 'bar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exploring Data in a DataFrame\n",
    "#The first thing you probably want to do is see what the data looks like. Here a few ways to check out Pandas data.\n",
    "\n",
    "df.head()       # first five rows\n",
    "df.tail()       # last five rows\n",
    "df.sample(5)    # random sample of rows\n",
    "df.shape        # number of rows/columns in a tuple\n",
    "df.describe()   # calculates measures of central tendency\n",
    "df.info()       # memory footprint and datatypes\n",
    "\n",
    "# The number of columns. Equal to df.shape[0]\n",
    "len(df) \n",
    "\n",
    "# An array of the column names\n",
    "df.columns \n",
    "\n",
    "# Columns and their types\n",
    "df.dtypes\n",
    "\n",
    "# Converts the frame to a two-dimensional table\n",
    "df.values\n",
    "\n",
    "df.cov() #covariance The cov method provides the covariance between suitable columns.\n",
    "df.corr() #correlation The corr method provides the correlation between suitable columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding a New Column to a DataFrame\n",
    "#The quick and easy way is to just define a new column on the dataframe. This will give us column with the number 23 on every row. Usually, you will be setting the new column with an array or Series that matches the number of rows in the data.\n",
    "\n",
    "df['new_column'] = 23\n",
    "#Need to build a new column based on values from other columns?\n",
    "\n",
    "full_price = (df.price + df.discount)\n",
    "df['original_price'] = full_price\n",
    "#Need the column in a certain order? The first argument is the position of the column. This will put the column at the begining of the DataFrame.\n",
    "\n",
    "df.insert(0, 'original_price', full_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sorting a DataFrame by a Certain Column\n",
    "df.sort_values('price', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We can also sort by one or multiple columns:\n",
    "\n",
    "df.sort_values(by=['col2', 'col1'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply a Function to Every Row in a Column\n",
    "def calculate_taxes(price):\n",
    "    taxes = price * 0.12\n",
    "    return taxes\n",
    "\n",
    "df['taxes'] = df.price.apply(calculate_taxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add a New Column with Conditional Logic\n",
    "df['profitable'] = np.where(df['price']>=15.00, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding the Mean or Standard Deviation of Multiple Columns or Rows\n",
    "df['mean'] = df.mean(axis=1)\n",
    "#or to find the standard deviation vertically\n",
    "\n",
    "df.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting a DataFrame to a Numpy Array\n",
    "#Converting the the values in a DataFrame to an array is simple\n",
    "\n",
    "df.values\n",
    "#If you want to preserve the table presentation\n",
    "\n",
    "df.as_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combining DataFrames with Concatenation\n",
    "#You can concatenate rows or columns together, the only requirement is that the shape is the same on corresponding axis. To concat rows vertically:\n",
    "\n",
    "pd.concat([df_1, df_2], axis=0)\n",
    "#Or to concat columns horizontally:\n",
    "\n",
    "pd.concat([df_1, df_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combining DataFrames based on an Index Key\n",
    "#you can perform inner, outer, left, right joins just like you would in SQL.\n",
    "\n",
    "merged_df = df_1.merge(df_2, how='left', on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge and Join\n",
    "#Pandas supports database-like joins which makes it easy to link data frames.\n",
    "\n",
    "In [57]: df\n",
    "Out[57]:\n",
    "   float_col  int_col str_col\n",
    "0        0.1        1       a\n",
    "1        0.2        2       b\n",
    "2        0.2        6    None\n",
    "3       10.1        8       c\n",
    "4        NaN       -1       a\n",
    "\n",
    "In [58]: other = DataFrame({'str_col' : ['a','b'], 'some_val' : [1, 2]})\n",
    "\n",
    "In [59]: other\n",
    "Out[59]:\n",
    "   some_val str_col\n",
    "0         1       a\n",
    "1         2       b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The inner, outer, left and right joins are show below. The data frames are joined using the str_col keys.\n",
    "In [60]: pd.merge(df,other,on='str_col',how='inner')\n",
    "Out[60]:\n",
    "   float_col  int_col str_col  some_val\n",
    "0        0.1        1       a         1\n",
    "1        NaN       -1       a         1\n",
    "2        0.2        2       b         2\n",
    "\n",
    "In [61]: pd.merge(df,other,on='str_col',how='outer')\n",
    "Out[61]:\n",
    "   float_col  int_col str_col  some_val\n",
    "0        0.1        1       a         1\n",
    "1        NaN       -1       a         1\n",
    "2        0.2        2       b         2\n",
    "3        0.2        6    None       NaN\n",
    "4       10.1        8       c       NaN\n",
    "\n",
    "In [62]: pd.merge(df,other,on='str_col',how='left')\n",
    "Out[62]:\n",
    "   float_col  int_col str_col  some_val\n",
    "0        0.1        1       a         1\n",
    "1        NaN       -1       a         1\n",
    "2        0.2        2       b         2\n",
    "3        0.2        6    None       NaN\n",
    "4       10.1        8       c       NaN\n",
    "\n",
    "In [63]: pd.merge(df,other,on='str_col',how='right')\n",
    "Out[63]:\n",
    "   float_col  int_col str_col  some_val\n",
    "0        0.1        1       a         1\n",
    "1        NaN       -1       a         1\n",
    "2        0.2        2       b         2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorization is a powerful tool. For almost all data-intensive computing, you will use numpy arrays rather than Python lists.\n",
    "\n",
    "#You've seen a similar concept in a spreadsheet where you add an entire column to another one.\n",
    "\n",
    "# we'll use numpy's array function to vectorize lists\n",
    "# we can do this on the fly:\n",
    "a=np.array([1,2,3,4,5])\n",
    "print a\n",
    "print type(a)\n",
    "\n",
    "# or we can reference a previously defined list:\n",
    "L=[2,4,6,8,10]\n",
    "b=np.array(L)\n",
    "print b\n",
    "print type(b)\n",
    "\n",
    "# now we can use vector math to combine the two\n",
    "print a+b\n",
    "print a-b\n",
    "print a*b\n",
    "print a/b\n",
    "print a**b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting Dates to their own Day, Week, Month, Year Columns\n",
    "#First, make sure the data is in datetime format. Then use dt method to extract the data you need.\n",
    "\n",
    "date = pd.to_datetime(df.date)\n",
    "df['weekday'] = date.dt.weekday\n",
    "df['year'] = date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding NaNs in a DataFrame\n",
    "#Count the total number of NaNs present:\n",
    "\n",
    "df.isnull().sum().sum()\n",
    "#List the NaN count for each column:\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filling NaNs or Missing Data\n",
    "#Most machine learning algorithms do not like NaN values, so you’ll probably need to convert them. If the topping column is missing some values, we can fill them a default value.\n",
    "\n",
    "df.topping = df.topping.fillna('Cheese')\n",
    "#or we can drop any row missing data across the entire DataFrame:\n",
    "\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extracting Features by Grouping Columns\n",
    "#Grouping columns is a great way to extract features from data. This is especially useful when you have data that can be counted or quantified in some way. For example, you might have group pizzas by topping, then calculate the mean for price in each group.\n",
    "\n",
    "df.groupby('topping')['discount'].apply(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#or maybe you want to see the count of a certain value\n",
    "\n",
    "df.groupby('topping')['discount'].apply(lambda x: x.count())\n",
    "topping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating Bins\n",
    "#Let’s say we want to create 3 separate bins for different price ranges. This is especially useful for simplifying noisy data.\n",
    "\n",
    "bins = [0, 5, 15, 30]\n",
    "names = ['Cheap', 'Normal', 'Expensive']\n",
    "\n",
    "df['price_point'] = pd.cut(df.price, bins, labels=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a new Column by Looping\n",
    "#Let’s say we want to categorize toppings by ‘vegetable’ or ‘meat’. Dealing with nominal values like these can be handled with a for loop. (Note: you can also use the apply function described earlier to perform this task. )\n",
    "\n",
    "topping_type = []\n",
    "\n",
    "for row in df.topping:\n",
    "    if row in ['pepperoni', 'chicken', 'anchovies']:\n",
    "        topping_type.append('meat')\n",
    "    else:\n",
    "        topping_type.append('vegetable')\n",
    "\n",
    "df['topping_type'] = topping_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Massive Datasets in Smaller Chunks\n",
    "#Sometimes you might have a massive file that will max out your RAM and crash your system. In that case, you might need to analyze the file in smaller chunks.\n",
    "\n",
    "chunksize = 500\n",
    "chunks = []\n",
    "for chunk in pd.read_csv('pizza.csv', chunksize=chunksize):\n",
    "    # Do stuff...\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting/Querying\n",
    "#Individual columns can be selected with the [] operator or directly as attributes:\n",
    "\n",
    "# Selects only the column named 'col1';\n",
    "df.col1 \n",
    "\n",
    "# Same as previous\n",
    "df['col1'] \n",
    "\n",
    "# Select two columns\n",
    "df[['col1', 'col2']]\n",
    "\n",
    "#You can also select by absolute coordinates/position in the frame. Indices are zero based:\n",
    "\n",
    "# Selects second row\n",
    "df.iloc[1]\n",
    "# Selects rows 1-to-3\n",
    "df.iloc[1:3]\n",
    "# First row, first column\n",
    "df.iloc[0,0]\n",
    "# First 4 rows and first 2 columns\n",
    "df.iloc[0:4, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Most often, we need to select by a condition on the cell values. To do so, we provide a boolean array denoting which rows will be selected. The trick is that pandas predefines many boolean operators for its data frames and series. For example the following expression produces a boolean array:\n",
    "\n",
    "# Produces and array, not a single value!\n",
    "df.col3 > 0\n",
    "#This allows us to write queries like these:\n",
    "\n",
    "# Query by a single column value\n",
    "df[df.col3 > 0] \n",
    "\n",
    "# Query by a single column, if it is in a list of predefined values\n",
    "df[df['col2'].isin(['Gold', 'Silver'])] \n",
    "\n",
    "# A conjunction query using two columns\n",
    "df[(df['col3'] > 0) & (df['col2'] == 'Silver')] \n",
    "\n",
    "# A disjunction query using two columns\n",
    "df[(df['col3'] > 0) | (df['col2'] == 'Silver')]\n",
    "\n",
    "# A query checking the textual content of the cells\n",
    "df[df.col2.str.contains('ilver')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One can select multiple boolean operators (| for or, & for and, and ~ for not) and group them by parenthisis.\n",
    "df[(df['float_col'] > 0.1) & (df['int_col']>2)]\n",
    "\n",
    "df[(df['float_col'] > 0.1) | (df['int_col']>2)]\n",
    "\n",
    "\n",
    "df[~(df['float_col'] > 0.1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modifying Data Frames\n",
    "#Pandas’ operations tend to produce new data frames instead of modifying the provided ones. Many operations have the optional boolean inplace parameter which we can use to force pandas to apply the changes to subject data frame.\n",
    "\n",
    "#It is also possible to directly assign manipulate the values in cells, columns, and selections as follows:\n",
    "\n",
    "# Modifies the cell identified by its row index and column name\n",
    "df.at[1, 'col2'] = 'Bronze and Gold' \n",
    "\n",
    "# Modifies the cell identified by its absolute row and column indices\n",
    "df.iat[1,1] = 'Bronze again' \n",
    "\n",
    "# Replaces the column with the array. It could be a numpy array or a simple list.\n",
    "#Could also be used to create new columns\n",
    "df.loc[:,'col3'] = ['Unknown'] * len(df) \n",
    "\n",
    "# Equivalent to the previous\n",
    "df.col3 = ['Unknown'] * len(df) \n",
    "\n",
    "# Removes all rows with any missing values.\n",
    "df.dropna(how='any') \n",
    "\n",
    "# Removes all rows with all missing values.\n",
    "df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It is often useful to create new columns based on existing ones by using a function. The new columns are often called Derived Characteristics:\n",
    "\n",
    "def f(x):\n",
    "    return x + ' New Column';\n",
    "\n",
    "# Uses the unary function f to create a new column based on an existing one\n",
    "df.col4 = f(df.col3) \n",
    "\n",
    "def g(x, y):\n",
    "    return x + '_' + y\n",
    "\n",
    "# Uses the 2-arg function g to create a new column based on 2 existing columns\n",
    "df.col4 = g(df.col3, df.col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dates and Time\n",
    "#When loading data from a CSV, we can tell pandas to look for and parse dates. The parse_dates parameters can be used for that. In the most typical case, you would pass a list of column names as parse_dates:\n",
    "dates_df = pandas.read_csv('test.csv', sep=';', parse_dates=['col1', 'col2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This will work for most typical date formats. If it does not (i.e. we have a non-standard date format) we need to supply our own date parser:\n",
    "def custom_parser(s):\n",
    "    # Specify the non-standard format you need\n",
    "    return pandas.datetime.strptime(s, '%d%b%Y')\n",
    "\n",
    "dates_df = pandas.read_csv('test.csv', sep=';', parse_dates=['col1'], date_parser=custom_parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Alternatively, if we’ve already loaded the data frame we can change a column from string to a date:\n",
    "dates_df['col2'] = pandas.to_datetime(dates_df['col2'], format='%d.%m.%Y')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Often we need to work with Unix/Posix timestamps. Converting numeric timestamps to pandas timestamps is easy with the unit parameter:\n",
    "\n",
    "# Unit specifies if the time is in seconds('s'), millis ('ms'), nanos('ns') etc.\n",
    "dates_df['col'] = pandas.to_datetime(dates_df['col'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If we need to parse Posix timestamps while reading CSVs, we can once again resort to converter functions. In the converter function we can use the pandas.to_datetime utility which accepts a unit parameter:\n",
    "\n",
    "def timestamp_parser(n):\n",
    "    # Specify the unit you need\n",
    "    return pandas.to_datetime(float(n), unit='ms')\n",
    "\n",
    "dates_df = pandas.read_csv('test.csv', sep=';', parse_dates=['col1'], date_parser=timestamp_parser)\n",
    "#We can also convert time/timestamp data to Unix epoch numbers:\n",
    "\n",
    "# Creates a new numeric column with the timestamp epoch in nanos\n",
    "dates_df.col4 = pandas.to_numeric(dates_df.col3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map, Apply\n",
    "#Forget writing for loops while using pandas. One can do beautiful vectorized computation by applying function over rows and columns using the map, apply and applymap methods.\n",
    "\n",
    "#map\n",
    "#The map operation operates over each element of a Series.\n",
    "\n",
    "df['str_col'].dropna().map(lambda x : 'map_' + x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply\n",
    "#The apply is a pretty flexible function which, as the name suggests, applies a function along any axis of the DataFrame. The examples show the application of the sum function over columns. (Thanks to Mindey in the comments below to use np.sum instead of np.sqrt in the example)\n",
    "df.ix[:,['int_col','float_col']].apply(np.sqrt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#applymap\n",
    "\n",
    "#The applymap operation can be used to apply the function to each element of the DataFrame.\n",
    "\n",
    "def some_fn(x):\n",
    "   ....:   if type(x) is str:\n",
    "   ....:     return 'applymap_' + x\n",
    "   ....:   elif x:\n",
    "   ....:     return 100 * x\n",
    "   ....:   else:\n",
    "   ....:     return\n",
    "   ....:\n",
    "\n",
    "In [40]: df.applymap(some_fn)\n",
    "Out[40]:\n",
    "   float_col  int_col     str_col\n",
    "0         10      100  applymap_a\n",
    "1         20      200  applymap_b\n",
    "2         20      600        None\n",
    "3       1010      800  applymap_c\n",
    "4        NaN     -100  applymap_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GroupBy\n",
    "#The groupby method let’s you perform SQL-like grouping operations. The example below shows a grouping operation performed with str_col columns entries as keys. It is used to calculate the mean of the float_col for each key. For more details, please refer to the split-apply-combine description on the pandas website.\n",
    "# http://pandas-docs.github.io/pandas-docs-travis/\n",
    "#http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods\n",
    "\n",
    "In [41]: grouped = df['float_col'].groupby(df['str_col'])\n",
    "\n",
    "In [42]: grouped.mean()\n",
    "Out[42]:\n",
    "str_col\n",
    "a           0.1\n",
    "b           0.2\n",
    "c          10.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### New Columns = f(Existing Columns)\n",
    "Generating new columns from existing columns in a data frame is an integral part of my workflow. This was one of the hardest parts for me to figure out. I hope these examples will save time and effort for other people.\n",
    "\n",
    "I will try to illustrate it in a piecemeal manner – multiple columns as a function of a single column, single column as a function of multiple columns, and finally multiple columns as a function of multiple columns.\n",
    "\n",
    "multiple columns as a function of a single column\n",
    "I often have to generate multiple columns of a DataFrame as a function of a single columns.\n",
    "\n",
    "http://manishamde.github.io/blog/2013/03/07/pandas-and-python-top-10/\n",
    "\n",
    "https://stackoverflow.com/questions/12356501/pandas-create-two-new-columns-in-a-dataframe-with-values-calculated-from-a-pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In [43]: df4 = df.copy()\n",
    "\n",
    "In [44]: def two_three_strings(x):\n",
    "   ....:   return x*2, x*3\n",
    "   ....:\n",
    "\n",
    "In [45]: df4['twice'],df4['thrice'] = zip(*df4['int_col'].map(two_three_strings))\n",
    "\n",
    "In [46]: df4\n",
    "Out[46]:\n",
    "   float_col  int_col str_col  twice  thrice\n",
    "0        0.1        1       a      2       3\n",
    "1        0.2        2       b      4       6\n",
    "2        0.2        6    None     12      18\n",
    "3       10.1        8       c     16      24\n",
    "4        NaN       -1       a     -2      -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### single column as a function of multiple columns\n",
    "It’s sometimes useful to generate multiple DataFrame columns from a single column. It comes in handy especially when methods return tuples. \n",
    "\n",
    "https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe?lq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n [47]: df5 = df.copy()\n",
    "\n",
    "In [48]: def sum_two_cols(series):\n",
    "   ....:   return series['int_col'] + series['float_col']\n",
    "   ....:\n",
    "\n",
    "In [49]: df5['sum_col'] = df5.apply(sum_two_cols,axis=1)\n",
    "\n",
    "In [50]: df5\n",
    "Out[50]:\n",
    "   float_col  int_col str_col  sum_col\n",
    "0        0.1        1       a      1.1\n",
    "1        0.2        2       b      2.2\n",
    "2        0.2        6    None      6.2\n",
    "3       10.1        8       c     18.1\n",
    "4        NaN       -1       a      NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### multiple columns as a function of multiple columns\n",
    "Finally, a way to generate a new DataFrame with multiple columns based on multiple columns in an existing DataFrame. \n",
    "https://stackoverflow.com/questions/10751127/returning-multiple-values-from-pandas-apply-on-a-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In [51]: import math\n",
    "\n",
    "In [52]: def int_float_squares(series):\n",
    "   ....:   return pd.Series({'int_sq' : series['int_col']**2, 'flt_sq' : series['float_col']**2})\n",
    "   ....:\n",
    "\n",
    "In [53]: df.apply(int_float_squares, axis = 1)\n",
    "Out[53]:\n",
    "   flt_sq  int_sq\n",
    "0    0.01       1\n",
    "1    0.04       4\n",
    "2    0.04      36\n",
    "3  102.01      64\n",
    "4     NaN       1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pandas Concatenation Tutorial\n",
    "https://www.dataquest.io/blog/pandas-concatenation-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
